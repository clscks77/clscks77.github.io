---
title: "[Note] 2025-05-05 어린이날에 p2p editing 공부하기"
categories: [Notepad, Daily]
tags: [vision, diffusion, study]
date: 2025-05-05 15:55:22 +0900
comments: false
excerpt: "■ (수업과제) 전체적인 발표 흐름 정리 ■ P2P 논문을 위한 백그라운드 공부 ■ (개인연구) Text-to-Text Diffusion 분야 서치"
--- 
---


함께 공부한 GPT: <https://chatgpt.com/c/6818a6b7-5ad0-800b-b38b-21e8e6833722>

---

## 전체적인 발표 흐름 정리


이미지 생성과 텍스트 기반 이미지 편집의 발전 흐름을 중심으로 논문 몇 편을 간결하게 짚어준다면?

1. **텍스트-이미지 공동 임베딩의 시작**
- CLIP (Radford et al., 2021)
- → 텍스트와 이미지를 동일한 표현 공간에 매핑함으로써, 자연어로 이미지 내용을 이해하거나 조작할 수 있는 기반을 마련.
- → CLIP은 이후의 대부분의 텍스트 기반 이미지 생성/편집 연구의 기반이 되는 모델입니다.


2. **GAN 기반 텍스트-기반 이미지 편집**
- StyleCLIP (Patashnik et al., 2021)
- → GAN의 잠재 공간을 조작하여, 텍스트를 기반으로 이미지 스타일이나 속성을 변경.
- → 단점: 얼굴 이미지와 같은 제한된 도메인에서만 안정적으로 작동.


3. **Diffusion 모델의 등장과 강력한 생성 성능**
- DDPM (Ho et al., 2020)
- → 이미지 생성을 위해 노이즈를 점차 제거하는 과정을 사용한 모델.
- → 이후의 DALL·E 2, Imagen, Stable Diffusion 등의 기반이 됨.


4. **대규모 텍스트-이미지 생성 모델**
- DALL·E 2 (Ramesh et al., 2022)
- Imagen (Saharia et al., 2022)
- → 자연어에서 매우 고품질의 이미지를 생성.
- → 하지만 편집 기능은 부족, 텍스트 조금만 바꿔도 전혀 다른 이미지 생성.


5. **마스크 기반 텍스트 편집 시도**
- GLIDE (Nichol et al., 2022)
- → 마스크를 통해 특정 영역만 변경 가능.
- → 단점: 사용자가 직접 마스크를 지정해야 하므로 직관성이 떨어짐.


6. **마스크 없는 텍스트 기반 편집 시도**
- Bar-Tal et al. (2022), "Text-based Editing of Images without Mask"
- → 마스크 없이 텍스트만으로 국소 편집을 시도.
- → 하지만 구조 변경엔 약하며, 입력 이미지마다 훈련이 필요함.


7. **Prompt-to-Prompt (Hertz et al., 2022)의 기여**
- → 기존 문제를 해결하기 위해, cross-attention map을 이용한 텍스트 기반 편집 제안
- → 마스크나 훈련 없이도 구조 보존 + 의미 변경 가능


| 단계 | 논문                                                            | 역할                            |
| -- | ------------------------------------------------------------- | ----------------------------- |
| 1  | CLIP (Radford et al., 2021)                                   | 텍스트-이미지 공동 임베딩                |
| 2  | StyleCLIP (Patashnik et al., 2021)                            | 텍스트 기반 GAN 이미지 편집             |
| 3  | DDPM (Ho et al., 2020)                                        | Diffusion 모델 기반 이미지 생성        |
| 4  | DALL·E 2 (Ramesh et al., 2022), Imagen (Saharia et al., 2022) | 고품질 텍스트-이미지 생성                |
| 5  | GLIDE (Nichol et al., 2022)                                   | 마스크 기반 텍스트 편집                 |
| 6  | Bar-Tal et al. (2022)                                         | 마스크 없는 편집 시도                  |
| 7  | Prompt-to-Prompt (Hertz et al., 2022)                         | cross-attention을 통한 텍스트 기반 편집 |


---

## P2P Image Editing 논문을 위한 백그라운드 공부

- P2P 논문의 백본으로 쓰이는 Imagen 같은,
- Diffusion 모델은 텍스트 임베딩(텍스트 prompt)과 '이미지의 noisy 상태에서의 latent representation'을 cross-attention을 통해 결합하여, 다음 상태의 이미지를 점진적으로 생성함

- 이때
    - Query Q = φ(zt) = 이미지 feature 
    - Key & Value K, V = ψ(P) = 텍스트 임베딩
    - Cross-attention map M = softmax(QK^T / √d)
        - M은 어떤 픽셀이 텍스트 prompt의 어떤 단어(token)에 주목해야 하는지를 weight로써 명시적으로 나타냄
    - 최종 output = M ## V

- P2P 논문은 원래 이미지에서 얻은 cross-attention map M을 diffusion 생성과정에 주입(inject) 함으로써, 
    - 텍스트를 바꾸더라도 구성(composition)이나 형상(geometry)을 최대한 유지하도록 제어할 수 있음을 보임

- 전체 알고리즘을 보면,
    - 원래 프롬프트 P, 수정된 프롬프트 P*에 대한 두 개의 diffusion 과정을 동시에 수행하고,
    - 매 step마다 attention map을 편집하여 **원하는 방식으로** 두 과정의 attention을 교차 적용함.
    - 결과적으로, 기존 이미지의 구성은 유지하면서 텍스트 프롬프트의 의미는 반영된 새로운 이미지를 생성할 수 있음.


### 실제 구현 상의 최적화 전략에 대한 설명



### diffusion 모델 내부에서 "텍스트와 이미지가 어떻게 만나는가?"에 대한 설명
- 텍스트(Text)와 이미지(Image)의 상호작용(interaction)이 정확히 어떤 구조를 통해 일어나는지

- Diffusion 모델은 각 단계에서 노이즈가 섞인 이미지 z_t에서 "다음 상태 z_{t-1}로의 노이즈 ε"을 예측하고 제거함
    - 이 noise prediction 단계에서, 텍스트 임베딩이 하난의 조건으로 들어가기 때문에, 텍스트가 이미지 생성에 영향(interaction)을 주는 시점이라고 한거
- 그리고 이 시점에서, 모델은 "이 픽셀은 어떤 단어를 참고해서 생성할지"를 학습하는데,
    - 이미지의 latent feature φ(z_t)가 텍스트 임베딩 ψ(P)을 참고해서 cross-attention layer에서 결합하는 방식으로 처리됨
    - Cross-attention의 결과물은 각 텍스트 토큰이 이미지의 어떤 공간(spatial location)에 영향을 주는지를 나타내는 attention map임
        - 예를 들어, "apple"이라는 단어에 대한 attention map은 사과가 위치할 이미지 영역에 집중될 것이고,
        - "red"는 사과 전체에 퍼져 있을 수도 있음


### 근데 U-shaped network [37] 이 논문은 왜 언급됐지? Imagen이 이 논문을 기반으로 했나?
- Imagen에서 사용하는 **noise prediction network(ε_theta)**가 U-Net 구조이기 때문
- U-Net은 이미지의 전체 정보(저해상도 의미 정보)와 세부 정보(고해상도 디테일)를 효과적으로 결합하기 위해 만들어짐 
- 거의 **모든 diffusion 모델 (DDPM, Stable Diffusion, Imagen 등)**이 노이즈 예측기 ε_theta를 U-Net 구조로 구성함
- Prompt-to-Prompt는 이 U-Net 내부의 cross-attention layer를 조작함으로써 작동함




### diffusion 모델 안에서 cross-attention이 수학적으로 정확히 어떻게 작동하는가

- scale을 위해 √d로 나눔 - 왜 root가 필요하지?
    - gradient vanishing/exploding 문제; Attention score가 너무 커지지 않게 조절해 softmax를 안정적으로 만들기 위해

- key, value가 prompt에서 갑자기 어떻게 생기는거지? 
    - 이건 Transformer의 Attention에서 나오는 개념
    - 프롬프트는 하나지만, Key/Value 역할을 하기 위해 **학습된 다른 선형 변환(linear projection)**을 통해 각각 다르게 쓰는 거

- 학습된 linear projection `𝓁_Q`, `𝓁_K`, `𝓁_V`가 아직 확실히 이해가 안됨
    - 이걸 어떻게 학습하는지가 궁금해

- Diffusion 모델에서 "noise prediction 오류"가 뭘 의미하는지, 그리고 loss가 어떻게 정의되는지
    - Forward process (정답 생성): 이미지에 노이즈를 추가해서 정규분포로 보내버리는 과정
        - 어떤 노이즈 ε가 추가되었는지를 알고 있음 → ground truth
    - Reverse process (학습 대상): 노이즈가 추가된 이미지에서 원래 이미지를 복원하는 과정
        - 노이즈가 추가된 이미지 x_t를 보고, 그 안에 숨어있는 노이즈 ε를 예측해라


attention이 잘 작동하도록 diffusion loss (ex. noise prediction 오류)를 줄이는 방향으로 전체 모델 학습 중에 같이 학습


### multi-head attention
- 하나의 attention head는 하나의 방식으로만 정보를 집중할 수 있음
- 이미지나 텍스트는 복잡한 구조를 가지므로, 여러 개의 head가 병렬로 각기 다른 방식으로 attention을 수행하게 함 (보통 8~12)
    - 예: 하나는 색깔, 하나는 모양, 하나는 위치에 주목할 수 있음
    - ℓ_Q^1, ℓ_Q^2, ..., ℓ_Q^h 이렇게 독립적인 linear projection을 사용하고, 독립적으로 attention 계산


- Imagen은 각 diffusion step에서 노이즈를 예측할 때, text prompt 정보를 조건(conditioning)으로 사용하는데,
- 이 conditioning은 attention layer 두 가지 방식으로 들어감
    1) cross-attention layers
        - pixel이 어떤 text에 주목할지 정하는 attention (앞에서 설명한 수식1)
    2) hybrid attention
        - self-attention + cross-attention
        - `K, V = concat(ℓ_K(x), ℓ_K(ψ(P)))     ← [image features | text embeddings]`
- 여기서 이미지끼리 주목하는 부분(self-attention)은 그대로 두고, 이미지가 텍스트를 참조하는 부분(cross-attention)만 조작하겠다는 거

----


### Self-supervised와 Zero-shot의 메커니즘 차이


----

## (개인연구) Text-to-Text Diffusion 분야 서치

### Text-to-Text Diffusion
- 전통적인 autoregressive 언어모델(예: GPT)과는 접근 방식이 다르다.
    - 일반적인 언어 모델(GPT, T5 등)은 다음 단어를 하나씩 순차적으로 생성합니다 → autoregressive
    - 반면, diffusion 모델은 전체 시퀀스를 노이즈로 만든 뒤, 점진적으로 깨끗한 텍스트로 복원합니다 → non-autoregressive

- Text-to-Text Diffusion 관련 연구
    - Diffusion-LM Improves Controllable Text Generation [NIPS, 2022] 최초로 제안된 text diffusion model
    - Analog Bits: Generating Discrete Data Using Diffusion Models (Austin et al., 2021)
        - 텍스트는 연속값이 아니라 이산적(discrete) → diffusion처럼 연속적인 노이즈 모델에 안 맞음
        - Gumbel-softmax 등으로 이산 값을 연속 공간으로 옮긴 뒤 diffusion 적용
    - Structured Diffusion for Text Generation (특정한 구조 (예: 문법적 구조, 서술 구조)에 따라 텍스트를 생성)

| 어려움                      | 설명                                        |
| ------------------------ | ----------------------------------------- |
| **텍스트는 이산적(discrete)**   | 이미지처럼 부드럽게 노이즈를 섞기가 어려움                   |
| **문법적 제약**               | 단어 순서와 문법이 엄격함 (중간 상태가 무의미할 수 있음)         |
| **속도**                   | diffusion은 보통 수백 step 필요 → **텍스트 생성엔 느림** |
| **fine-grained control** | 텍스트는 세밀한 수정이 어렵고 불연속적 변화가 큼               |



### diffusion 연구 방법
- cross-attention 구조만 바꿔 Prompt-to-Prompt 연구하기
- 학습 없이 embedding space 조작하는 연구 (e.g., attention injection)
- pre-trained U-Net을 부분적으로 수정한 연구


