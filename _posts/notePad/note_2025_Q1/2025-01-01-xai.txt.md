---
title: "[Note] 25-01-01 xai.txt"
categories: [Notepad, Daily]
tags: 
date: 2025-01-01 21:49:00 +0900
comments: false
---
---

CF에서 AI 0사용을 수용하는 데 있어 가장 중요한 요소 중 하나는 AI 시스템에 대한 신뢰를 구축하는 것입니다. 




XAI의 공식적인 정의는 섹션 1.2에 나와 있습니다. 이 섹션에서는 XAI의 이유, 목적, 방법에 대해 더 자세히 살펴봅니다. 

AI의 보편적 존재는 AI가 얼마나 강력한지를 결정하며 우리 삶의 일부가 되었습니다. 특히 보건 분야[34, 35]와 법의학 분야[36]에서 중요한 결정을 내리는 데 적용되고 있습니다.

이러한 분야에서 AI가 직면한 특정 문제가 있습니다. 주요 문제 중 하나는 신뢰입니다. 

예를 들어, 법의학 분석가가 AI 시스템에서 생성된 보고서를 통해 아동 착취 사건의 용의자를 식별할 때입니다. 
이 용의자가 증거를 가지고 법정에 출석할 때, 법원은 이러한 결과를 바탕으로 판결을 내릴 수 있을까요? 

이러한 결과를 신뢰하기 위해 법원은 이러한 결과에 대한 설명을 요구하며, 이 결과에 도달하는 데 사용되는 AI 방법을 해석하고 설명해야 합니다. 

이는 사이버 범죄 수사 중 법의학 분석의 핵심 요건 중 하나로 받아들여져 법정에 제출되어야 합니다. \\

XAI의 주요 목적은 AI에 대한 신뢰를 개발하고 성공적인 법적 CF 분석에서 중심적인 역할을 하는 것입니다. 
\


설명을 생성하는 데는 세 가지 기본 접근 방식이 있습니다: 
(1) 의사 결정 트리 및 기타 규칙 기반 접근 방식. 
이러한 모델의 출력은 시스템 사용자에게 설명하기 쉬운 작은 규칙 세트에 따라 달라집니다. 

(2) 특징 선택 및 분석, 작은 특징 세트에 중점을 둡니다. 이는 모델이 복잡하더라도 입력과 출력 간의 관계를 쉽게 이해할 수 있게 합니다. (3) 가장 일반적인 접근 방식은 외부 메커니즘을 사용하여 블랙박스 AI 시스템의 내부 작동을 설명합니다. 이 접근 방식의 방법 중 하나는 의사 결정 트리와 같은 프록시 모델을 생성하여 원래 모델을 이해하고 설명합니다. 또 다른 방법은 모델을 다른 입력으로 쿼리하여 어떤 입력이 중요한지 평가하는 것입니다. 이 접근 방식 내에서 인기 있는 XAI 알고리즘은 LIME(로컬 해석 가능한 모델-불가지 설명) [37]과 SHAP(SHAPley Additive Explanation, 39)입니다. 국립표준기술연구소는 네 가지 원칙[40]을 도입하고 모든 XAI 시스템이 이러한 원칙을 준수해야 한다고 제안했습니다.






1. 설명 - 시스템은 그 결과를 논리적이고 지능적이며 체계적으로 추론할 수 있다면 설명할 수 있습니다. 실제로 모든 AI 시스템은 스스로를 설명하는 방식이 다릅니다. 따라서 이 원칙을 일반화하기 위해 정확성에 대한 조건을 붙이거나 품질 지표를 부과하지 않습니다. XAI 모델에는 자체 설명 가능과 사후 설명 가능의 두 가지 유형이 있습니다. 자체 설명 가능은 이름에서 알 수 있듯이 스스로 설명합니다. 이 모델은 로컬뿐만 아니라 전역적으로도 설명할 수 있습니다. 입력에 대한 각 결정을 설명할 수 있으며, 이를 로컬 설명이라고도 하고 모든 입력에 대해 전체적으로 설명할 수 있으며, 이를 글로벌 설명이라고도 합니다. 가장 일반적인 자체 설명 가능 모델은 의사 결정 트리와 회귀 모델입니다. 이러한 모델은 가장 단순한 AI 모델 중 일부이며 인간에게 더 의미 있게 설계되었습니다. 

사후 설명 가능은 이름에서 알 수 있듯이 결과물을 본 후에 설명합니다. 이러한 설명은 다른 소프트웨어 도구에 의해 렌더링되며, 입력에 대해 출력을 쿼리할 수 있는 경우 알고리즘의 내부 작동을 알지 못해도 알고리즘에 사용할 수 있습니다. 
이러한 설명은 기능 중요도 점수, 규칙 세트, 히트맵 또는 자연어 표현을 생성합니다. 
이를 통해 사용자는 기능의 중요성을 정량화하고 모델에 의해 생성된 의사 결정을 설명할 수 있습니다. 

일부 연구자[41, 42, 43]는 단순성과 정확성의 상충 관계를 주장하며 자체 설명 가능한 모델이 사후 설명 모델보다 정확도가 떨어진다고 말합니다. 이에 동의하지 않는 다른 연구자[35, 44, 45]도 있습니다. 


2. 의미 있는 - AI 시스템은 그 설명이 이해할 수 있는 것이라면 의미 있는 것입니다. AI 시스템의 이해관계자는 이 원칙에서 중요한 역할을 합니다. 설명이 의미 있는지는 이해관계자의 지식, 경험, 시스템과의 관계 및 기타 다양한 심리적 요인에 따라 달라집니다. 또 다른 주요 요소는 설명의 목적인데, 예를 들어 법적 포렌식의 경우와 비즈니스 목적의 설명은 다를 수 있습니다. 이를 통해 의도한 청중이 이해할 수 있는 설명이 만들어집니다. 다양한 청중에게 의미 있는 설명을 제공하는 데에는 몇 가지 어려움이 있습니다. 예를 들어 법의학자가 배심원과 같은 일반인에게 증거를 설명한다고 가정해 보겠습니다. 이러한 설명은 일반적으로 이해하기 어렵고 일반 독자에게 오해를 불러일으킬 수 있으므로 의미 있는 설명의 기준을 충족하지 못합니다[46]. 의미 있는 설명을 생성하기 위한 몇 가지 과제[46, 47, 48]는 의미 있는 설명에 대한 사용자의 개인차, 설명의 맥락, 다양한 설명 유형에 대한 사용자의 기대치입니다. XAI 시스템은 맥락에 적합하고 의도한 사용자가 이해할 수 있는 의미 있는 정보를 제공해야 합니다. 


3. 설명 정확도 - AI 시스템이 결론에 도달하는 과정을 정확하게 설명하는 설명의 능력입니다. 이는 AI 알고리즘 및 시스템의 정확도와는 다릅니다. 설명의 세부 수준도 중요합니다. 어떤 청중은 간단한 설명으로 충분하지만, 어떤 청중은 결과를 더 잘 이해하기 위해 더 자세한 설명이 필요할 수 있습니다. 발명가는 동료들에게 자신의 발명을 상당한 기술적 세부 사항과 함께 설명할 수 있습니다. 같은 설명이라도 교육을 받지 않은 친구나 부모에게는 다르게 전달될 가능성이 높습니다. 설명의 정확성은 특정 청중에게 얼마나 의미가 있는지에 따라 달라집니다. 따라서 의미와 설명의 정확성은 서로 연관되어 있습니다. 수신기 작동 특성 곡선 및 곡선 아래 면적[56], 정밀도-리콜 곡선[57], 비용 곡선[58] 등과 같이 AI 시스템의 정확도를 측정하는 지표[49, 50, 51, 52, 53, 54, 55]가 있습니다. 이 외에도 XAI 시스템의 성능을 평가하기 위한 메트릭을 개발하는 작업[59, 60, 61, 62]도 진행 중입니다. XAI 시스템의 정확도는 어떻게 측정하나요? 설명이 얼마나 좋은지(정확하고 명확한지), 해당 청중이 얼마나 만족하는지, 설명이 얼마나 신뢰할 수 있는지(정당한지)와 같은 특정 기준이 있습니다. XAI 시스템의 평가 방법에 대한 자세한 분류법은 [62]에 나와 있습니다. 



4. 지식 한계 – 이 원칙은 AI 시스템이 시스템의 지식과 설계의 한계 내에서 작동하도록 보장합니다. 이러한 한계를 벗어난 모든 작동이나 결과는 잘못된 판단을 초래하므로 신뢰할 수 없습니다. 이는 시스템이 오해의 소지가 있는 출력/판단을 무시하는 데 도움이 되며, 따라서 AI 시스템에 대한 신뢰를 높이는 데 도움이 됩니다. AI 시스템이 지식 한계를 초과할 수 있는 방법은 두 가지가 있습니다. 하나는 쿼리나 작업이 AI 시스템의 도메인 밖에 있는 경우입니다. 예를 들어, CF에서 성공적인 파일 조각 분류는 이미지 자체가 아닌 파일에 이미지가 내장된 경우, 예를 들어 PDF 파일에 내장된 이미지가 있는 경우(63) AI 알고리즘에게 어려운 문제가 됩니다. 이 경우 AI 알고리즘이 아무런 설명 없이 조각을 PDF 파일이 아닌 이미지로 분류하면 AI 알고리즘을 사용하는 시스템은 이를 오해의 소지가 있는 출력으로 간주할 수 없습니다. 그러나 AI 알고리즘이 그 한계를 알고 설명하면 AI 시스템은 이 오해의 소지가 있는 출력을 무시할 수 있습니다. 두 번째는 출력의 신뢰도가 너무 낮은 경우입니다. 위의 예를 다시 살펴보면, 여러 파일 유형) 조각을 다중 클래스로 분류하는 것도 어려운 문제입니다(63). 이 경우 AI 알고리즘이 조각을 두 개 이상의 클래스로 분류하고 이 중 하나의 클래스를 출력으로 무작위로 선택하면 이는 오해의 소지가 있는 출력이 됩니다. 이러한 한계를 알고 있는 AI 시스템은 이러한 출력을 신뢰할 수 없습니다.


4.1.1. XAI 방법의 분류. 
그림 4와 같이 설명의 범위에 따라 XAI 방법을 나누고 분류합니다. 설명의 범위는 그 범위와 범위를 의미합니다. 로컬 또는 글로벌일 수 있습니다. 로컬 및 글로벌 모두 내재적 방법일 수 있으며 사후적 방법일 수 있습니다. 정의에 따르면 내재적 방법은 모델에 따라 다르며 사후적 방법은 일반적으로 모델에 구애받지 않습니다. 이 섹션에서는 인기 있는 로컬 및 글로벌 방법 중 일부를 소개합니다. 이 특정 주제에 대한 포괄적인 커버리지는 독자를 참조하십시오 [23, 64, 65].


1. 로컬 설명 - 이러한 설명의 범위는 단일 입력 또는 입력의 하위 집합입니다. 가장 일반적인 것은 단일 입력 포인트의 출력에 대한 설명입니다. 로컬 설명 알고리즘으로는 LIME [37], SHAP [38, 39], 살리언스 맵 [66, 67], 카운터팩츄얼 [68] 등이 있습니다. 
LIME은 선형 모델이나 의사 결정 트리와 같은 자체 해석 가능한 프록시 모델을 구축하여 원본 모델의 각 예측을 설명합니다. 
이 새로운 모델은 원본 데이터의 해석 가능한 표현을 포함하는 새로운 데이터 세트에 대해 학습됩니다. 
원래 모델은 새 모델의 특징에 부여된 가중치로 설명할 수 있습니다. 

예를 들어 한 모델이 환자가 독감에 걸렸다고 예측한다고 가정해 보겠습니다. 재채기와 두통이 없는 경우보다 더 많은 가중치를 부여하는 등 상대적인 가중치로 증상을 강조하여 설명합니다. 의사는 이러한 설명을 통해 쉽게 판단할 수 있습니다. 

SHAP은 또한 원래 모델의 해석 가능한 근사치인 더 간단한 모델을 구축하여 사용합니다. 게임 이론을 기반으로 각 특징에 대한 샤플리 값을 계산하여 특징 중요도(예측 결과에 대한 각 특징의 기여도)를 제공합니다. 

민감도 맵 또는 히트 맵이라고도 하는 중요도 맵은 예측 결과에 미치는 영향력에 따라 각 특징에 대한 속성 값을 계산하여 각 예측을 설명합니다. 중요도 맵은 기여도를 히트 맵으로 시각화할 수 있는 모델의 시각적 설명입니다. 

역설적 설명은 입력의 변화가 출력을 수정할 수 있는 정도를 나타냅니다. 
목표는 새로운 입력 값을 최소화하고 가능한 한 원래 입력에 가깝게 만드는 것입니다. 목표는 로컬 검색을 통해 이 목적 함수를 최대화하는 것입니다. 이 함수는 엔드포인트 손실, 즉 해당 값이 원하는 클래스에 얼마나 가까운지, 원점과 새 값 사이의 거리에 따라 달라집니다. 역설적 설명은 적대적 예제와 유사하지만 목적이 다릅니다. 분류기를 회피하는 것이 목적인 적대적 예제와 달리 설명을 제공함으로써 모델에 대한 이해를 돕습니다. 

2. 전역 설명 - 이러한 설명의 범위는 전체 알고리즘입니다. 글로벌 설명 알고리즘으로는 PDP(부분 의존도 플롯) [69], ICE(개별 조건 기대치) [70], GSA(글로벌 민감도 분석) [71, 72], SP-LIME(서브모듈러 픽 라임) [37] 등이 있습니다. PDP 설명은 특징값에 변화가 있을 때 출력의 한계 변화를 나타냅니다. PDP는 특정 인스턴스가 아닌 전체 평균 인스턴스에 초점을 맞춥니다. 이는 특징과 예측 결과 사이의 관계를 보여줍니다. ICE 플롯은 피처와 예측된 응답 사이의 함수 관계를 나타내는 선형 차트가 있는 그래프입니다. 다른 모든 피처는 고정된 상태로 유지되며 분석 중인 피처의 값만 변경됩니다. GSA 방법은 입력 피처의 값 범위를 교란하여 주어진 모델의 예측에 미치는 영향을 정량화합니다. LIME의 변형인 SP-LIME은 가장 관련성이 높은 로컬 LIME 설명을 선택해 글로벌 설명을 요약하고 제공합니다.







민감도 맵 또는 히트 맵이라고도 하는 saliency 맵은 예측 결과에 미치는 영향력에 따라 각 특징에 대한 속성 값을 계산하여 각 예측을 설명







4.2. XAI의 지난 10년
 독자들이 본 논문에서 수행된 연구에 익숙해질 수 있도록 지난 10년간 XAI에서 수행된 작업에 대한 비판적이고 간략한 리뷰를 제시합니다. 

CF에서의 XAI 적용에 대해서는 섹션 6에서 자세히 논의할 예정이므로 이 논문에서는 제외했습니다. 이 주제에 대한 자세한 조사는 이 백서의 범위를 벗어납니다. 자세한 조사는 관심 있는 독자들이 [27, 73]을 참고하시기 바랍니다. 
[73] S. Ali, T. Abuhmed, S. El-Sappagh, K. Muhammad, J. M. Alonso-Moral, R. Confalonieri, R. Guidotti, J. Del Ser, N. D´ ıaz
Rodr´ıguez, F. Herrera, Explainable artificial intelligence (xai): What we know and what is left to attain trustworthy
 artificial intelligence, Information Fusion 99 (2023) 101805.

★Wang 등[74] (2015)은 의료 애플리케이션에서 의사 결정을 위해 설계된 해석 가능한 예측 모델의 일종인 폴링 룰 리스트(FRL)를 학습하기 위한 베이지안 프레임워크를 소개했습니다. FRL은 만약-그렇다면 규칙의 정렬된 목록으로 구성되며, 고위험 환자를 먼저 분류하는 데 우선순위를 둡니다. 
베이지안 접근법은 해석 가능성을 향상시켜 투명한 의사 결정 프로세스를 제공하며, 이는 고위험 의사 결정에서 신뢰와 이해의 중요성이 커지고 있는 의료 상황에서 유용할 수 있습니다.  

Su 등[75] (2015)은 2단계 부울 규칙 학습을 위해 최적화 기반 알고리즘을 사용하여 분류 정확도와 규칙 단순성 간의 균형을 맞추는 데 중점을 두었습니다. 
도입된 알고리즘은 분류 정확도와 인간의 해석 가능성을 모두 고려하여 우수한 성능을 보여줌으로써 AI 상황에서 향상된 성능으로 해석 가능한 모델을 만드는 데 기여했습니다.
 
★Letham 등[76] (2015)은 예측 정확도와 해석 가능성 사이의 균형을 맞추기 위해 의사 결정 목록에 대한 생성 모델인 베이지안 규칙 목록(BRL)을 도입했습니다. 이 방법은 규칙의 순열에 대한 계층적 선행에 의해 구동되며, 정확성과 해석 가능성을 모두 제공하는 높은 사후 확률을 가진 의사 결정 목록을 생성합니다. 이는 개인화된 의료 분야에서 해석 가능한 모델의 잠재력을 보여주며, 도메인 전문가와의 효과적인 커뮤니케이션을 위해 간결하고 신뢰할 수 있는 규칙의 중요성을 강조합니다.

Hara 등[77] (2016)은 더 간단하고 사람이 해석할 수 있는 모델로 근사화하여 부가 트리 모델(ATM)[78] ★의 해석 가능성을 향상시키는 후처리 방법을 제안합니다. 
ATM이 일반적으로 생성하는 많은 수의 영역이 해석 가능성을 저해한다는 문제가 해결되었습니다. 

미슈라 등[79] (2017)은 시간, 주파수, 시간-주파수 세분화에 기반한 세 가지 버전의 설명을 도입하여 LIME 기법을 음악 콘텐츠 분석(MCA)으로 확장하여 SLIME이라는 이름을 붙였습니다. SLIME은 모델 행동에 대한 인사이트를 제공하여 높은 정확도에도 불구하고 의사 결정 트리 모델의 일반화에 대한 한계를 드러내고 많은 경우 모델 의존적 중요도 맵과 일치하는 신경망에 대한 모델 무관적 설명을 보여줍니다. 

★Fong 등[80] (2017)은 분류기 결정에 가장 책임이 있는 이미지 영역을 식별하는 데 중점을 두고 모든 블랙박스 알고리즘에 대한 설명을 학습하기 위한 모델 불가지론적 프레임워크를 제안합니다. 제안된 방법은 명시적이고 해석 가능한 이미지 섭동을 사용하여 휴리스틱 기반 이미지 살리니티 방법의 한계를 해결합니다. 이 프레임워크는 해석 가능하고 테스트 가능한 설명에 기여하여 신경망의 취약성과 인공물에 대한 취약성에 대한 통찰력을 보여줍니다. ★ ICCV

★Ribeiro 등[81] (2018)은 복잡한 모델의 동작을 설명하기 위해 if-then 문 형태의 고정밀 규칙을 제공하는 Anchors라는 모델 독립적 설명 시스템을 제안합니다. 앵커는 모든 블랙박스 모델에 대한 설명을 높은 확률로 효율적으로 계산하여 다양한 영역과 작업에서 유연성을 보여줍니다. 사용자 연구에 따르면 앵커를 사용하면 기존의 모델에 구애받지 않는 설명 기법에 비해 더 높은 정확도와 적은 노력으로 보이지 않는 인스턴스에 대한 모델 동작을 예측할 수 있습니다. ★ AAAI

Guidotti 등[82] (2018)은 특정 사례에 대해 해석 가능하고 충실한 설명을 제공하는 데 중점을 두고 블랙박스 결정을 설명하는 모델 불가지론적 방법인 LORE(로컬 규칙 기반 설명용)를 소개합니다. LORE는 유전 알고리즘을 사용하여 합성 이웃을 생성한 다음 의사 결정 트리의 형태로 해석 가능한 로컬 예측자를 구축합니다. 결과 설명에는 의사 결정 규칙과 사실과 반대되는 규칙 세트가 포함됩니다. LORE는 블랙박스를 모방할 때 설명 품질과 정확성 측면에서 기존 방법보다 뛰어난 성능을 발휘하여 의사 결정 시스템의 투명성 부족이라는 윤리적 문제를 해결합니다. 

★Joo 등[83] (2019)은 아타리 게임의 맥락에서 Grad-CAM을 심층 강화 학습(DRL)에 적용하는 방법을 조사합니다. 이 연구는 CNN 계층의 중요성을 밝혀냄으로써 인공지능 에이전트가 아타리 게임을 플레이하면서 의사 결정을 내리는 방식에 대한 이해를 높이고 Grad-CAM을 DRL에 적용하여 이해를 높였습니다.

Gramegna 등[84] (2020)은 손해보험 영역에서 고객의 의사 결정을 해독하기 위한 XAI 모델을 제안합니다. 저자들은 행동 세분화를 강화하는 샤플리 값과 XGBoost를 통합하여 보험 업계에서 고객 행동에 대한 이해를 개선하는 설명 가능한 머신러닝의 잠재력을 보여줍니다. 

Cavaliere 등[85] (2020)은 복잡한 머신러닝 모델, 특히 의료 애플리케이션에서 설명력이 떨어지는 문제를 해결했습니다. 
이들의 연구는 자동 정의 함수가 있는 문법 진화(GE)를 활용하여 파킨슨병 자동 감지에 초점을 맞췄습니다. 

★파사 외[86] (2020)는 신속한 사고 감지를 통해 교통 안전을 강화하기 위해 익스트림 그라디언트 부스팅(XGBoost) 머신러닝 기법을 적용했습니다. 
이 연구는 높은 정확도와 감지율로 XGBoost의 효과를 입증했으며, SHAP 분석을 통해 교통 관련 특징, 특히 속도 변화의 중요성을 강조했습니다. 

★Feichtner 등[87] (2020)은 안드로이드 애플리케이션에서 개발자가 정의한 애플리케이션 동작과 권한 사용 간의 불일치를 식별하는 것을 목표로 합니다. 설명적 텍스트로 훈련된 심층 신경망을 활용하여 권한 점수를 예측하고 설명 가능성 분석에 LIME을 사용합니다.  

De 등[88] (2020)은 의사 결정 트리 생성 알고리즘인 Cluster-TREPAN과 숨겨진 계층 클러스터링을 결합하여 심층 신경망(DNN)이 내린 결정의 해석 가능성을 향상시키는 방법을 제안합니다. 신경망의 예측을 설명하는 데 중점을 두고 해석 가능성과 신뢰성의 중요성을 강조합니다. 
제안된 Cluster-TREPAN 방식은 DNN 내의 정보 흐름을 효과적으로 포착하고 설명하여 딥러닝 모델을 인간이 수용하는 데 중요한 측면을 해결함으로써 성능 면에서 LIME보다 뛰어납니다. 

★Szczepa´nski 등[89] (2021)은 실제 시나리오에서 모델 결정을 이해하기 위한 중요한 필요성을 해결하면서 BERT 기반 가짜 뉴스 탐지기의 설명 가능성을 향상시키는 데 중점을 두었습니다. 이 연구는 대리 유형 설명 방법, 특히 LIME과 앵커를 활용하여 언어 기반 가짜 뉴스 탐지 모델에서 해석 가능성을 향상시킬 수 있는 가능성을 입증했습니다. 

★Gite1 외[90] (2021)는 주가 예측을 위해 효율적인 머신러닝 기법, 특히 장단기 기억(LSTM)의 고급 조합을 활용합니다. 
이 논문에서는 모델의 의사 결정 과정에 대한 투명하고 의미 있는 인사이트를 얻기 위해 LIME과 함께 XAI 요소를 도입했습니다. 

Naeem 등[91] (2022)은 딥 러닝 기법을 활용하고 Grad-CAM을 사용하여 IoT 디바이스에서 자동화된 멀웨어 탐지의 중요한 문제를 해결하고 전반적인 성능을 이해하고 개선합니다. 

★Sutton 등[92] (2022)Sutton 등은 8,000개의 내시경 이미지를 활용하여 궤양성 대장염의 진단 정밀도를 향상시키기 위해 Grad-CAM 기술을 적용합니다. 
Grad-CAM의 선택은 모델 학습에 대한 시각적 통찰력을 제공하지만, 사용된 AI 기술과 사용된 특정 XAI 방법에 대한 보다 자세한 논의는 연구의 기여도에 대한 포괄적인 평가를 제공합니다. 

Debjit 외[93] (2022)의 연구는 코로나19 조기 탐지를 위한 고급 머신러닝 프레임워크를 제시하며, 해리스 호크스 최적화 알고리즘을 활용하여 ML 분류기의 하이퍼파라미터와 모델 해석을 위한 SHAP 값을 최적화합니다. 이 연구는 SHAP을 활용하여 특징의 중요성을 밝혀 투명성을 제공하고, 코로나19에 대한 의료 모니터링 영역에서 중요한 임상적 인사이트에 기여합니다. 

Nordin 등[94]  (2023)은 랜덤 포레스트 및 그라디언트 부스팅과 같은 복잡한 앙상블 학습 모델과 SHAP 방법을 결합하여 자살 시도에 대한 설명 가능한 예측 모델을 제안합니다. 이 접근 방식은 자살 시도 예측의 신뢰도를 높이는 동시에 임상 의사 결정에 중요한 요소에 대한 인사이트를 제공하는 것을 목표로 합니다. 이 연구는 정신 건강 분야에서 정확도 측면에서 랜덤 포레스트보다 SHAP을 사용한 그라데이션 부스팅의 우월성을 강조하며 의료 분야에서 설명 가능한 머신 러닝 기술의 가치를 강조합니다.



4.2.1. 요약.
성장하는 XAI 분야에서 지난 10년 동안 머신러닝 모델의 복잡성을 풀기 위한 노력이 크게 급증했습니다. 
연구자들은 베이지안 프레임워크, 최적화 기반 알고리즘, 모델에 구애받지 않는 기법을 사용하여 의료 및 금융에서 사이버 보안에 이르기까지 다양한 영역을 탐구해 왔습니다. 
이러한 접근 방식은 높은 모델 정확도를 달성하는 것과 해석 가능성을 보장하는 것 사이에서 미묘한 균형을 맞추기 위해 노력합니다. 
연구 커뮤니티는 LIME, SHAP, 앵커와 같은 방법을 도입하여 복잡한 모델의 의사결정 과정을 이해하기 위해 적극적으로 노력하고 있습니다. 
이러한 노력은 모델의 전반적인 성능을 향상시킬 뿐만 아니라 의사 결정 시스템의 투명성과 관련된 윤리적 문제를 해결하여 고급 AI 시스템의 기능에 대한 이해와 신뢰를 높일 수 있을 것으로 기대됩니다. 
이러한 노력은 특히 의료 모니터링, 주가 예측, 사이버 보안과 같은 애플리케이션에서 두드러지게 나타나며, 
LIME 및 SHAP과 같은 XAI 요소의 통합은 모델 결정에 대한 투명한 인사이트를 제공합니다. 
AI 기술은 계속 발전하면서 모델 하이퍼파라미터를 최적화할 뿐만 아니라 다양하고 복잡한 실제 시나리오에서 기능의 중요성을 밝히고 의사결정에 중요한 인사이트를 제공하는 데 중추적인 역할을 하고 있습니다. 
궁극적으로 XAI의 발전은 머신러닝 모델을 실제 애플리케이션에 보다 책임감 있고 신뢰할 수 있게 통합하는 데 기여합니다. 
이 섹션에서 검토한 논문의 출처 및 연도별 분포는 그림 5에 나와 있으며, 사용된 기술과 유형에 대한 분포는 표 2에 나와 있습니다.




4.3. 사이버 포렌식(CF).
CF에 대한 공식적인 정의는 섹션 1.2에 나와 있습니다. 이 섹션에서는 왜, 무엇을, 어떻게 CF를 하는지 자세히 살펴봅니다. 
가해자의 모든 접촉은 흔적을 남깁니다[95]. 가해자를 상대로 소송을 제기하려면 이러한 흔적이나 증거를 찾아 수집, 확보, 연구, 분석해야 합니다. 
사이버 포렌식[3, 96, 97]은 과학적 방법과 전문 지식을 사용하여 법정에서 범죄 또는 기타 조사에 사용할 수 있는 사이버 기기에서 발견되는 증거를 수집하고 분석합니다.
이러한 증거는 전자 증거 개시, 정보 및 행정 등 다양한 목적으로 사용될 수 있습니다. 
예를 들어 디지털 장치에서 수집한 데이터는 실행 가능한 인텔리전스를 제공할 수 있습니다. 이러한 정보는 국익 보호, 납치 및 아동 착취와 같은 범죄의 감소 또는 근절 등 다양한 유형의 임무를 달성하는 데 도움이 될 수 있습니다. 전자 증거개시는 나중에 민사 또는 형사 포렌식 사건에서 사용할 전자 데이터를 검색, 발견, 확보하는 프로세스입니다. CF 프로세스의 주요 목적은 해당 사건과 관련된 세부 사항과 사실을 찾고 분석하여 관심 있는 사건을 더 잘 이해하고 결론을 내리는 것입니다. 일반적으로 이 프로세스는 그림 6과 같이 4단계[98]로 구성되며 아래에 설명되어 있습니다: 

1. 수집 - 증거 수집은 CF에서 가장 중요한 작업입니다. 데이터를 수집하는 동안 해당 데이터를 보존하고 보호해야 할 필요성이 매우 높습니다. 수집된 증거는 우발적이거나 고의적인 손상으로부터 보호되어야 합니다. 휘발성 데이터가 손실되지 않는다는 보장이 있는 즉시 사이버 기기에 접근할 수 없도록 해야 합니다. 또한 데이터의 동적 특성으로 인해 수집은 즉시 수행되어야 합니다. 데이터를 즉시 수집하지 않으면 배터리로 구동되는 장치와 네트워크 연결에서 데이터가 손실될 수 있습니다. 수집 단계의 작업은 잘 문서화되어야 합니다. 의심되는 사이버 디바이스의 포렌식 복제본 또는 이미지를 만들고 복제된 디바이스에서 검사를 수행합니다. 보관 체인을 유지하고 문서화해야 합니다. 포렌식 과정에서 수집된 증거가 변경되거나 변경되지 않았는지 확인하기 위해 적절한 보관 체인을 유지해야 합니다.

2. 검사 - 자동 및 수동 방법/기술을 사용하여 데이터(크든 작든)를 포렌식적으로 면밀히 조사하는 프로세스입니다. 이 과정에서 특별히 관심 있는 데이터를 추출하고 분석하는 동시에 데이터의 무결성을 보존합니다. 또한, 정보가 포함된 데이터 파일이 정의되는데, 이 파일은 압축되거나 암호화되거나 액세스 제어로 보호될 수 있어 의미 있는 추출이 어렵습니다. AI 기술과 도구를 사용할 수 있게 되면서 유용한 데이터를 선별하고 추출하는 것이 훨씬 쉬워졌습니다. 또한 분석해야 하는 데이터의 양도 줄어듭니다. 이러한 AI 프로세스/기술을 신뢰하려면 설명할 수 있어야 하며, 이는 CF에서 XAI의 적용을 더욱 설득력 있게 만듭니다. 

3. 분석 - 이 단계에서는 법적으로 정당한 방법과 기술을 사용하여 심사 결과를 분석합니다. 이 단계의 목적은 법정에서 사용하고 제시할 수 있는 유용한 정보를 데이터에서 추출하는 것입니다. 데이터의 상관관계, 느린 네트워크 트래픽 등 데이터의 이상 징후, 자동화된 기술(AI, ML, 데이터 마이닝 등)과 수동 기술을 사용하여 데이터를 선별하여 공격을 예측하는 등 다양한 기법이 사용됩니다. 여기서 AI의 활용은 매우 유용합니다. 이 프로세스를 자동화할 뿐만 아니라(현재는 부분적으로) 데이터에서 의미 있는 정보를 추출하는 데도 도움이 됩니다. 그러나 법정이나 대중이 이러한 정보를 받아들이려면 이러한 정보를 추출하는 과정을 설명할 수 있어야 하므로 CF에 XAI를 적용하는 것이 더욱 필수적입니다. 

4. 보고 - 마지막 단계이지만 포렌식 프로세스에서 가장 중요한 단계이기도 합니다. 분석 단계의 결과를 기술자와 비기술자 모두가 보다 포괄적이고 이해하기 쉽게 만듭니다. 일반적으로 포렌식 프로세스 중에 사용된 작업, 사용된 도구/방법, 기타 수행해야 할 작업으로 구성되며, 포렌식 프로세스를 개선하기 위한 권장 사항과 지침을 제공할 수도 있습니다. 정확한 설명을 제공하기는 어렵지만 독자가 사건에 대해 책임감 있고 책임감 있는 결론과 결정을 내릴 수 있도록 가능한 한 많은 세부 정보를 제공해야 합니다. 여기서 XAI는 포렌식 분석가가 이러한 보고서를 작성하는 데 사용할 수 있는 사람이 이해할 수 있는 설명을 제공하는 데 중요한 역할을 할 수 있습니다.


// 검사랑 분석이랑 뭐가 다른거임
// Collection; 수집 (일단 싹 모아오기) → Examination; 검사 (유용해 보이는 데이터 선별 및 추출) → Analysis; 분석 (추출된거 분석) → Reporting; 보고 (이해하기 쉽게 결과보고서 작성)
	// 이건가봐




포렌식은 증거가 사이버 기기에 있는 범죄를 해결하는 데 중요한 도구입니다[3]. 
처음에는 포렌식 기술이 주로 데이터 복구를 위해 개발되었습니다. 
기술이 발전함에 따라 네트워크 및 메모리 포렌식이 가능해졌고 CF가 널리 보급되고 신뢰도가 높아졌습니다. 
이제 스마트폰, IoT 등과 같은 복잡한 사이버 디바이스의 등장, 운영체제와 파일 형식의 확산, 만연한 암호화, 원격 처리 및 저장을 위한 클라우드 사용, 법적 문제 등으로 인해 CF는 많은 새로운 도전에 직면해 있습니다. 
예를 들어, 스마트폰에는 각각 수백만 개 이상의 애플리케이션을 다운로드할 수 있는 수십 개의 시스템이 실행되고 있습니다. 파일 카빙은 스마트폰과 같은 사이버 디바이스에서 원시 데이터를 추출하여 파일로 분류하는 기술 중 하나입니다. 이렇게 많은 양의 원시 데이터를 분류하여 데이터를 이해하기 위해서는 AI 분야와 같은 새로운 기술이 필요합니다[63]. 
이러한 기술을 CF에 성공적으로 적용하려면 그 결과를 사람에게 정당화하고 설명해야 합니다[16]. 
여기서 의문은 XAI가 그러한 설명 가능한 결과를 제공할 수 있는가 하는 것입니다. 이것이 이 백서에서 수행한 연구의 주요 초점입니다.




4.3.1. 분류 .
기본적인 CF 기법은 동일하지만 모바일, IoT 등과 같은 특정 환경에서만 적용되는 기법도 있습니다. 
이 섹션에서는 포렌식 과정 중 이러한 작업 환경을 기반으로 CF의 주요 분류에 대해 설명합니다. 
모든 사이버 장치는 일종의 컴퓨터 장치, 즉 전자 데이터를 처리하고 저장할 수 있는 장치입니다. 
따라서 컴퓨터 포렌식을 CF에서 별도의 클래스로 간주하지 않습니다. 
이 백서에서는 이 클래스를 일반으로 명명하며 여기서는 이 클래스만을 설명하지 않습니다.

...
PASS


4.4.1. 요약. 
검토된 논문들은 사이버 범죄의 증가에 대응하기 위해 디지털 증거를 체계적으로 수집, 분석, 보존하는 데 필수적인 분야인 CF 분야에서 끊임없이 진화하는 데 크게 기여하고 있습니다. 

다양한 CF 영역을 아우르는 이 논문은 분산 파일 시스템의 신뢰성 향상을 위한 블록체인 활용, 출처 증명 및 암호화를 통한 클라우드 포렌식의 복잡한 과제 해결, 포렌식 건전성을 위한 로그 형식 통일 개선 등 정교한 분야를 심층적으로 다룹니다. 이러한 다양한 기여는 디지털 포렌식 환경에서 방법론, 기술, 도구가 지속적으로 발전하고 있음을 강조하며 사이버 범죄 수사 및 예방 전략의 효율성을 보장하는 데 중추적인 역할을 하고 있음을 보여줍니다. 이 논문에서 얻은 인사이트는 연구자들이 데이터 무결성, 개인정보 보호, 사이버 위협의 역동적인 환경과 같은 복잡한 문제와 씨름하는 CF의 다면적인 특성을 강조합니다. 디지털 영역이 계속 확장됨에 따라 CF에 대한 혁신적인 접근 방식의 필요성이 점점 더 분명해지고 있습니다. 이 논문들은 사이버 범죄자보다 앞서 나가기 위해 필요한 학제 간 노력을 종합적으로 강조하며, CF 연구의 경계를 넓히고 안전하고 신뢰할 수 있는 디지털 환경을 유지하는 데 중요한 역할을 강화합니다. 이 섹션에서 검토한 논문의 출처 및 연도별 분포는 그림 7에 나와 있으며, 포렌식 유형(클래스)에 따른 분포는 표 3에 나와 있습니다.



// 이제 본론이다
5. Explainable Artificial Intelligence in Cyber Forensics (XAI-CF).

5.1. 왜 XAI-CF인가?
인공지능 시스템은 사용자의 입력 없이도 학습하고 적응할 수 있기 때문에 여러 업무 분야에서 다양한 작업을 자동화하고 완료하는 데 유용합니다. 
또한 대량의 데이터를 처리할 수 있는 능력은 방대한 양의 데이터를 다루는 CF에 사용하기에 이상적입니다[127]. 
AI 기반 기술을 이용한 증거 처리 자동화는 CF 분석 프로세스를 신속하게 처리하는 데 유망한 결과를 보여줍니다[4]. 
AI 시스템의 내재적 복잡성으로 인해 주요 문제 중 하나는 이해하고 해석하기가 쉽지 않으며, 인간이 AI 시스템이 어떻게 결정을 내렸는지 깨닫기 어렵다는 것입니다. 
이 문제는 이러한 시스템이 법정에서 증거를 법의학적으로 분석하고 판단하는 형사 사건의 판결처럼 중요한 결정을 내리는 데 사용될 때 더욱 심각해집니다. 

여기서 우리는 전문가뿐만 아니라 CF 분야에 대한 사전 지식이나 전문 지식이 없는 사용자도 해석하고 설명할 수 있는 AI 모델을 구축하는 방법에 대한 중요한 질문과 문제를 제기합니다. 

일반적으로 CF의 이해관계자는 AI 전문가가 아닙니다. 
이들이 정보에 입각한 의사결정을 내리려면 먼저 해당 AI 시스템의 결과물을 이해해야 합니다. 
CF에서 XAI를 사용하면 CF 분석을 개선하고 법의학적으로 중요한 증거를 CF의 다양한 이해관계자가 해석하고 이해하기 쉬운 방식으로 추출하여 수사 및 잠재적으로 법정에서 성공적으로 사용할 수 있습니다. 
우리는 이미 디지털 시대에 접어들었으며, CF가 이를 최대한 활용하고 직면한 과제[4, 128]를 성공적으로 해결하려면 CF 프로세스에 AI를 완전히 도입해야 합니다. AI를 CF에 성공적으로 도입하려면 AI 시스템의 결과를 해석하고 CF의 이해관계자에게 설명해야 합니다. 의사결정 과정에 관여하는 CF 이해관계자에게 AI 결과를 설명하기 위해 여러 연구[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147]가 수행되었습니다. 그 중 일부는 섹션 6에서 자세히 설명합니다.

5.2. 어떻게 XAI-CF .
AI라고 하면 [5] 사람이 수행할 때 지능이 필요한 기능을 수행하는 기계와 같은 지능과 의사 결정, 문제 해결, 학습 등과 같은 자동화의 두 가지를 떠올립니다. AI는 보편적인 분야[5]로 지식 표현, 적대적 검색, 통계 및 확률적 추론, 지도, 비지도 및 강화 학습, 자연어 처리 등 다양한 하위 분야, 방법 및 기법을 포괄합니다. CF용 AI 시스템에서는 이러한 기법 중 하나 또는 조합을 사용하여 CF 분석을 수행하고 결정(출력)에 도달합니다. CF용 XAI 시스템에서는 이러한 기법 중 하나 또는 조합을 사용하여 AI 시스템의 결정(출력)을 해석하고 설명하기도 합니다. 

XAI 시스템은 표 4와 같이 불투명, 해석 가능, 이해 가능, 진정한 설명 가능의 네 가지 범주[148]로 나눌 수 있습니다. 
처음 세 가지 범주는 현재 XAI-CF에 대한 작업을 포괄합니다. 
마지막 범주는 CF 프로세스의 중요한 특성과 결정을 이해하기 위한 인간의 일관되지 않은 추론 과정으로 인해 XAI-CF에서 달성하기가 매우 어렵습니다. 

휴먼 인 더 루프[30, 149]는 XAI-CF 모델이 진정한 설명 가능성을 달성하는 데 도움이 될 수 있습니다. 
여기에서는 XAI-CF를 달성하기 위한 몇 가지 기법에 대해 설명합니다. 표 6은 섹션 6에서 논의한 XAI-CF에 대한 현재 및 이전 연구에서 이러한 기법을 강조합니다.


5.2.1. 모델 단순화.
 XAI 모델에서 사용하는 기본 접근 방식 중 하나는 원래 모델을 단순화하는 것입니다. 단순화된 모델은 해석, 이해 및 설명하기 쉽습니다. 이를 달성하기 위한 다양한 기법이 있습니다. 사용되는 기법 중 하나는 대리 모델입니다[138]. 원래 모델의 예측을 사용하여 간단한 의사 결정 트리와 같은 대리 모델을 구축합니다. 이 투명한 모델은 특징이 예측에 미치는 영향을 알기 어려울 때 구축되며 원래 모델의 동작을 근사화하도록 학습됩니다. 대리 모델은 원본 모델의 의사 결정 과정을 이해하는 데 도움이 됩니다. 이러한 대리 모델의 문제점 중 하나는 원래 모델과 동일한 예측을 제공하지만 근거와 추론이 다를 수 있다는 것입니다. 이로 인해 포렌식 분석가 및 기타 CF 이해관계자가 대리 모델을 사용하지 않을 때와 다른 결정을 내리도록 유도할 수 있는 증거 간의 대조적인 관계가 형성될 수 있습니다. 이러한 한계를 해결하기 위해 연구자들은 XAI에서 충실한 대리 모델을 평가하는 메트릭을 개발하기 위해 [150] 노력하고 있습니다. 또 다른 기법으로 사람이 읽을 수 있는 형식으로 모델의 의미와 추론된 정보를 강조하는 스토리텔링[139]이 사용되고 있습니다. 포렌식 법적 증거 모델에 대한 추론을 공식화하고 단순화하는 데 사용되는 다른 기법[136]으로는 내러티브, 논증, 확률론적 접근 방식이 있습니다.

5.2.2. 기능 중요도/관련성
XAI-CF에서 가장 많이 사용되는 방법은 특징을 추출하고 모델을 구축한 다음 입력된 특징과 모델 출력의 중요도 및 관련성을 계산하여 모델을 해석하고 설명하는 것입니다. 이러한 지식이 있으면 모델의 견고성을 테스트하기 위해 모델에 다양한 입력을 만들 때 도움이 되며, 따라서 모델에 대한 신뢰도가 높아집니다. 특징 중요도는 모델을 해석하는 사람의 노력을 줄여주는 다양한 기술을 사용하여 모델을 설명하는 데 도움이 됩니다. 특징 관련성은 포렌식 분석에 가장 적합한 모델을 선택하는 데도 도움이 됩니다. 이러한 방법 중 하나[138]는 먼저 대리 모델을 사용하여 모델을 단순화한 다음 특징이 예측에 미치는 영향을 계산합니다. [139]는 모델에서 시간적 패턴을 채굴하고 스토리텔링을 사용하여 패턴을 연관시켜 사람이 읽을 수 있는 형식으로 정보를 출력하고 강조합니다. [142]는 LIME[37]을 사용하여 입력 특징의 관련성을 출력과 연관시키고 LIME이 포렌식 분석에 가장 적합한 모델을 선택하는 데 중요한 XAI 기법이라고 결론지었습니다.

5.2.3. 시각화.
 이 기법은 모델을 설명하는 가장 좋은 접근 방식이며 사람이 이해하기 쉽습니다. 포렌식 분석을 위한 대화형 데이터 기반 시각화 도구는 XAI-CF 시스템에서 매우 바람직합니다. 이는 모델의 투명성을 높이고 포렌식 분석가 및 기타 XAI-CF 이해관계자가 모델을 이해하고 추론하는 능력을 크게 향상시킵니다. 이러한 능력은 최종 의사 결정 과정에서 도움이 됩니다. 중요도 또는 히트 맵은 모델의 기여도를 시각적으로 설명합니다. 때로는 모델 그래프에 입력과 출력의 관계를 나타내는 기호로 태그를 지정하기도 합니다. 출력의 시각화는 포렌식 분석가 및 CF의 다른 이해관계자에게 모델을 해석하고 설명하는 데 큰 도움이 될 수 있습니다. 그러한 작업 중 하나[130]는 모델에서 추론 구조를 시각적으로 표현하는 인수 다이어그램(기호로 태그 지정)을 추출합니다. [147] 포렌식 분석을 위해 딥페이크 비디오 데이터를 시각화하고 해석합니다. 포렌식 분석가는 이미지의 일부를 확대하여 예측 점수에 대한 모델의 기여도를 관찰할 수 있습니다. 보시다시피 이 분야에는 개선의 여지가 많으며, 특히 비전문가를 위해 더 나은 설명을 위해 CF 모델을 시각화하는 새롭고 혁신적인 방법을 개발해야 할 필요성이 있습니다. 휴먼 인 더 루프는 이러한 시각화를 보완하여 모델을 더 잘 이해하고 설명할 수 있습니다. 


5.2.4. 휴먼-인-더-루프.
 휴먼 인 더 루프는 XAI-CF 시스템을 보완하여 가치 있고 실용적이며 구조화된 결과를 도출할 수 있습니다. 이는 진정으로 설명 가능한 시스템을 구현하는 데 가장 중요한 기술이라고 생각합니다. AI에 인간을 포함시키는 세 가지 접근 방식이 있습니다: AI가 인간보다 더 많은 제어권을 갖는 능동적 학습[151], 인간과 AI 간의 긴밀한 상호작용이 이루어지는 대화형 학습[152], 인간이 AI보다 더 많은 제어권을 갖는 기계 학습[153]. XAI-CF의 경우 중간 방식인 대화형 학습을 사용하는 것이 좋습니다. 이 방법을 권장하는 주된 이유 중 하나는 CF에서 사용할 수 있는 데이터의 특성 때문입니다. CF에 사용되는 대부분의 데이터는 파일 조각, 멀웨어 프로그램, 이미지, 비디오 등을 위한 하드디스크 원시 데이터와 같은 바이너리 형식입니다. 이러한 종류의 데이터는 반정형 또는 비정형 데이터의 범주에 속합니다. 이러한 원시 데이터를 AI가 패턴을 감지하거나 분류할 수 있는 적절한 내부 표현으로 변환하기 위해 대화형 학습은 이러한 원시 데이터에 추가적인 구조를 부여하는 데 유용합니다. 또 다른 주된 이유는 대화형 학습이 XAI-CF 시스템에서 제공할 수 있는 설명 가능성입니다. XAI-CF에서 학습 과정의 일부가 될 뿐만 아니라 모델이 학습한 내용을 최종적으로 해석할 수 있습니다. 학습에 포렌식 전문가를 추가하면 XAI-CF의 지식과 설명력을 향상시키는 데 도움이 됩니다. 대화형 학습에는 AI와 인간의 측면이 섞여 있고 인간(포렌식) 전문가에 의존해야 한다는 단점[154]도 있습니다. 현재 XAI에서 휴먼 인더루프의 역할을 연구한 연구는 충분하지 않습니다[155]. 최근 XAI-CF 분야에서 [149]는 전문 포렌식 분석가를 휴먼 인 더 루프로 사용하여 반복할 때마다 시스템이 개선되고 최종적으로 모델에 대한 더 나은 설명을 생성하도록 조정을 제공하도록 제안했습니다. 

5.3. 평가하기 .
XAI-CF AI 시스템을 평가하기 위한 지표가 있으며, XAI 시스템의 성능을 평가하기 위한 지표를 개발하는 작업이 진행 중입니다. XAI 시스템의 정확도는 어떻게 측정하나요? 설명이 얼마나 정확하고 명확한지, 해당 청중이 얼마나 만족하는지, 설명이 얼마나 신뢰할 수 있는지(정당한지) 등의 특정 기준이 있습니다. XAI 시스템의 평가 방법에 대한 자세한 분류법은 [62]에 나와 있습니다. 대부분의 선행 연구는 특정 XAI 모델에서만 작동하는 메트릭을 제안하거나[60, 156, 157, 158, 159, 160], 사람을 사용하여 XAI 모델의 유용성에 대해 연구하고 의견을 제시합니다[161, 162]. 세 가지 접근법[59, 60, 61]만이 모델 불가지론적이라고 주장하며 모든 XAI-CF 모델에 사용할 수 있습니다. 여기서는 이 세 가지 접근법에 대해 간략히 설명합니다. 호프만 등[59]은 XAI를 평가하는 세 가지 주요 기준, 즉 설명이 얼마나 정확하고 명확한지, 해당 청중이 얼마나 만족하는지, 설명이 얼마나 정당한지 등을 제시했습니다. 저자는 연구자와 전문가가 체크리스트, 특정 척도 및 심리측정법을 사용하여 이러한 기준을 수동으로 사용하여 XAI 모델을 평가할 것을 권장합니다. Rosenfeld 등[60]은 XAI를 정량화하기 위한 네 가지 객관적인 지표를 소개합니다. XAI의 목표가 주어지면 이러한 지표는 설명 자체와 설명의 적절성을 정량화합니다. 첫 번째 지표는 블랙박스 모델과 투명 모델의 성능을 비교합니다. XAI-CF는 법적 문제가 수반되기 때문에 이분법적인 평가를 요구합니다. 이 메트릭은 XAI-CF 모델을 평가하는 데 사용할 수 있으며, 임계값보다 큰 값은 설명을 무효화합니다. 두 번째 메트릭은 설명에 사용된 규칙의 수로 애플리케이션의 복잡성을 측정합니다. 설명이 단순할수록 더 바람직합니다. 세 번째 메트릭은 설명을 구성하는 데 사용된 기능의 수에 중점을 둡니다. 네 번째 메트릭은 설명의 안정성을 측정합니다. 이러한 모든 지표에는 임계값이 필요하며, 이는 특정 XAI 모델에 대한 사용자 연구를 수행하여 계산됩니다. 최근 소브라노 등[61]은 자연어로 기술된 설명의 정확성을 측정하는 지표 중 하나로 설명 가능성의 정도(얼마나)를 제안했습니다. 저자들은 범용 답변 검색을 위해 사전 학습된 심층 언어 모델을 사용하여 설명 가능성을 측정합니다[163, 164, 165, 166]. 의존성 파서를 사용하여 설명 텍스트 내의 모든 절을 감지하여 그래프를 추출합니다. 이러한 절은 주어, 본문, 목적어의 특수 삼중 항으로 표현됩니다. 사전 학습된 심층 언어 모델을 사용하여 이러한 삼중 항을 가능한 답변으로 자연어 표현을 얻습니다. 그런 다음 사전 정의된 기존 질문 세트에 대한 답변과의 관련성을 측정하여 설명 가능성의 정도를 정량화합니다.













