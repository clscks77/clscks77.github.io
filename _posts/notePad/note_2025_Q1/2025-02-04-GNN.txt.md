---
title: "[Note] 25-02-04 GNN.txt"
categories: [Notepad, Daily]
tags: 
date: 2025-02-04 21:49:00 +0900
comments: false
---
---

* GNN은 정보를 그래프로 표현했을 때, 노드 마다 정보의 feature를 나타낼 수 있는 representation(embedding)을 잘 만드는거고, 그 feature를 가지고 task를 잘 수행하면 됨
* 어떻게 그래프로 표현할거고, 어떻게 representation할건지, 그걸 연구하는게 핵심인 듯함
* 모델 작동의 원리와 프로세스를 완벽히 이해시키고, 그를 통해 베이스라인 고도화를 혼자서도 가능하게 만드는 것

- Deep Graph Representation Learning
- Graph attention network(GAT)
- Graph Convolutional Networks(GCN)
- Spectral-based graph convolution networks / Spatial-based graph convolution networks
- Malkov random fields(MRF)

____________________________________________________________________________________________________________

https://www.graphusergroup.com/

##1. 그래프 기초

1.
Centrality(중심성 지표)는 그래프 내에서 노드의 상대적인 중요도를 수치화함
Centrality에는 여러 종류가 있으며, 각각 다른 측면의 Node Importance를 나타냄
- Degree Centrality: 직접적인 이웃 노드 수에 비례
- Closeness Centrality: 특정 노드로부터 그래프 내의 모든 다른 노드까지의 최단 경로의 평균 길이를 계산
- Betweenness Centrality: 그 노드가 그래프 내의 최단 경로들에서 얼마나 자주 등장하는지 (브릿지 역할)
- Eigenvector Centrality: 그 노드와 직접적으로 연결된 이웃 노드들의 중요성으로 계산
	-- 이 방법에서 중요한 노드는 중요한 이웃을 많이 가진 노드
	-- Google의 PageRank 알고리즘에서 사용된 원리와 유사>?
	-- 그래프의 연결 구조가 두 개 이상의 연결 요소로 나뉘어져 있거나, 그래프가 bipartite (이분 그래프) 구조인 경우는 계산이 불가함

2.
Density(그래프의 밀도)
그래프의 복잡성 판단, 데이터의 특성 이해, 성능 향상에 중요한 역할을 함
밀도(Density)는 [실제 연결되어 있는 엣지/이론상 가능한 모든 엣지]의 비율
최대 밀도 1은 모든 노드가 서로 연결되어 있는 완전 그래프를 의미함

3.
Adjacency Matrix(인접 행렬)
그래프의 노드들이 서로 어떻게 연결되어 있는지를 나타내는 행렬
크기는 (노드 수) x (노드 수)
행렬 원소 값은 두 노드 간의 엣지 여부 또는 가중치



## 2. Random Walk, Deep Walk, Node2Vec
Random Walk 알고리즘으로 Adjacency Matrix의 Graph Structure information을 활용해보자

1.
Random Walk
Graph의 특정 Node에서 시작해 임의의 Direction(방향)으로 이동하는 과정 (그래프 상의 노드들을 무작위로 순회하는 가장 베이직한 방법론)
- Node 간의 Connectivity(연결성) 및 전체적인 연결 패턴 파악
- GNN에서는 *Node Representation을 학습할 때 랜덤 워크를 이용하는 경우가 많다고 (?)
노드의 단순 무작위 선택에서 함께 자주 나타나는 Node들은 서로 '가까운(closeness)' 관계라고 판단한다는 개념
- 여기서 가까운(closeness) 게 뭔지 정의해야 하는데,
- Network homophily hypothesis(네트워크 동질성 가정)과 관련이 있음. 쉽게 말해, 가까운 거리에 있는 Node들은 서로 비슷한 Attribute를 갖는다는 것
Random Walk의 개념을 차용해서 Local information을 사용하는 건, 전체 그래프를 분석하는 것보다 훨씬 효율적인 방법임
또한 Random Walk는 Node Embedding 생성에 사용되는 DeepWalk, Node2Vec같은 알고리즘의 기반임
- Node 사이의 경로를 sampling하는 데에 Random Walk를 사용한다?


 def random_walk(G, start_node, walk_length):
    walk = [start_node] # 시작 노드를 포함하는 리스트 walk를 생성
    # walk 리스트는 무작위 경로의 노드를 저장하는 데 사용
    
    for i in range(walk_length - 1):
        # 시작 노드가 이미 walk안에 있으니, -1.
        neighbors = list(G.neighbors(walk[-1]))
        if len(neighbors) == 0:
            break
        next_node = random.choice(neighbors)
        walk.append(next_node)
    return walk
# 그래프의 모든 노드에서 시작하는 많은 수의 랜덤 워크를 생성하는 코드로 개선해보자


2.
Deep Walk
"Graph Embedding"을 생성하는 알고리즘으로, Node 간의 Similarity Score(유사도 점수)를 측정하는 방식으로 작동함
- 서로 가까운 노드들의 유사도 점수를 높게, 멀리 떨어진 노드들의 유사도 점수는 낮게 설정함으로써 그래프 네트워크의 Structure Feature(구조적 특성)을 학습하고,
- 각 Node를 효과적으로 represent하는 low dimentional vector를 생성함
DeepWalk는 그래프 상의 노드들을 무작위로 순회하는 Random walk를 사용하여 "Node를 Vector Space에 투영"하는 방법론임
(= Random walk의 실행결과로 Node Sequence를 생성한거임)

3.
Node2Vec
단어들의 벡터 표현(임베딩)을 학습하는 데 사용되는 머신러닝 모델
비슷한 문맥에서 등장하는 단어들이 벡터 공간에서 가까이 위치하도록 단어들의 벡터를 조정하는 방식으로 작동해서, 최종적으로 모델 임베딩이 단어 사이의 의미적 관계를 반영하도록 함
- GNN은 생성된 시퀀스와 Word2Vec(특히 Skip-gram 아키텍처)으로 각 노드의 Vector Representation(=Embedding)을 학습함
---- 그래서 Word2Vec 학습을 어떻게 한다고?

Graph의 본질은 Relation이기에 우리는 어떤 노드부터 어떤 노드까지가 실제로 연관성이 있다고 판단할 것인지 정의해야 함
Task에 적합한 Sampling Strategy를 수립해야 한다는 것
Random Walk는 Node간의 연결 경로인 Edge를 랜덤하게 타고 돌아다니며, 어떤 Node 사이에 Relation이 있는지 Sampling한 것임
- p : return parameter. 값이 높을수록 본 적 없는 새로운 Node를 찾음 (=DFS처럼 동작)
- q : in-out parameter. 값이 높을수록 이웃이 아닌 노드로 이동하는 확률이 낮아짐 (=BFS처럼 동작)
파라미터 p와 q를 적절히 조정함으로써 random walk가 멀리 닿게 할 수도 있고, 타겟노드의 주변 노드들만을 돌아다닐 수도 있음
- random walk로 돌아다닌 범위까지를 연관성이 있는 노드로 치는거임
--- p가 높을 때가 성능이 좋다 = 유사한 특성을 가진 노드들이 서로 연결되는 경향이 있다 = homophilic한 컨셉을 가진 graph이다
 
4.
Vector Representation
노드를 고차원 벡터로 표현하는 것으로, 해당 Node의 Attribute을 캡쳐하는 것으로 해석할 수 있음
이를 통해 원래의 데이터 공간에서는 알아챌 수 없는 패턴을 발견할 수 있음
다시 체크, Deep Walk의 목표는 노드들의 Feature Representation(=Vector Representation, embedding)을 만들어내는 것

5.
skip-gram 아키텍처
중심 단어를 기반으로 주변 단어를 예측하는 방식으로 작동
주변 단어의 문맥을 이해하는 데에 특히 효과적


GNN의 포인트는 그래프를 구성하는 Node의 Attribute(=Feature)를 학습하는 것


6.
Structural Equivalence
두 개 이상의 노드가 유사한 연결 패턴을 가지고 있는지를 판단하는 데에 사용
- 두 노드의 neighbor nodes를 확인해 봤을 때, 겹치는 노드가 많다면 Structural Equivalent하다고 말할 수 있음


7.
Homophily
비슷한 특성을 가진 노드들은 서로 연결될 가능성이 높다는 개념
두 노드의 Attribute가 비슷하다면, 두 노드가 neighborhood일 가능성 (Edge로 연결되어 있을 가능성)이 높다


### Node2Vec 구현
https://www.graphusergroup.com/graph-travel-3/




## 3. Graph Convolution Network (GCN)
GCN은 그래프 데이터를 처리하기 위한 Convolutional Neural Network의 변형
- 각 노드는 이웃 노드의 정보를 집계함으로써 새로운 feature를 얻음
- 입력 계층과 숨겨진 계층에서는 각 계층 이후에 SELU 활성화 함수와 dropout이 적용
// 음, 논문을 읽거나 논문 리뷰를 보는게 낫겠는디

- {Node Representation, Message Passing, 그리고 Aggregation} = 학습 과정에서 노드끼리 정보를 교환한다

1.
GCN은 Target Node와 Neighbor Node 사이에서 정보를 교환하는 방식으로 작동
"노드 간 정보 교환"이란 한 노드가 이웃 노드들의 정보를 수집하고, 그 정보를 통해 새로운 Representation을 만들어내는 것 (=Embedding 업데이트)

degree matrix를 embedding에 곱하는 것은 각 노드의 이웃 정보들을 취합해 target node의 임베딩에 통합하는 과정의 일부분이다??

GCNs 모델은 convolution layer를 거칠 때마다 Node Representation(임베딩)을 업데이트함
- Node Representation은 각 노드의 feature vector와 이웃 노드들의 feature vector를 Aggreation하거나 mean하며 계산됨
- layer들은 input feature(node representation)를 입력받은 다음, 그것에 가중치를 곱하고, 그 결과에 non linear activation function을 적용해 새로운 representation을 업데이트함
- fully connected layer의 경우, 각 레이어는 이웃 노드 정보와 타겟 노드의 특성을 모두 고려해 각 노드의 임베딩을 업데이트 (?당연한 말을 반복하고이ㅏㅆ어)


2.
Message Passing
embedding 업데이트 프로세스

Message Passing Framework = 각 노드가 이웃 노드로부터 정보를 수집하고, 그 정보를 통해 node representation을 업데이트하는 것


3.
Neighborhood Aggreagtion
"GCNs 아키텍쳐가 Node Representation을 업데이트할 때, 
Neighborhood Aggregation을 통해 정보를 집계하고, 
그렇게 집계한 정보를 Message Passing으로 노드 간 교환한다"

이 정보교환으로 Graph Structure 통한 Node Representation을 학습하고, 복잡한 그래프 구조의 패턴을 인식하고 모델링 함
- 각 노드는 Adjacency Matrix를 통해 이웃 노드의 정보를 수집하고, Degree Matrix를 통해 이 정보를 정규화함


이렇게 GCN은 그래프 구조 정보를 적극적으로 활용하기 때문에, Graph Structure와 Node Representation이 중요하게 작용하는 Task에서 특히 뛰어난 성능을 발휘함
ex. GCNs 모델을 통해 Node Representation을 완성해 추출해 내고, 간단한 선형 신경망을 활용해 Edge의 연결 가능성을 예측




## 4. Graph Attention Network (GAT)

GAT는 Transformer 모델처럼 Attention Mechanism을 활용함
- 중요한건 "노드 정보를 취합하는 방법론"이라는 것
- Attention이란 개념이 어떻게 정보를 취합해내는지

(*핵심 개념)
1) Attention Mechanism
= 각 노드가 이웃 노드와 얼마나 연관이 있는지, 여러 노드 중 어떤 노드가 중요한지를 판단하는 방법론

2) Self Attention
= 노드 특성 정보를 업데이트할 때, 본인 노드의 정보도 고려하는 방법\

3) Multi Head Attention
= Attention Mechanism을 여러 개 병렬로 사용하여, 다양한 측면에서 정보수집을 하는 방법

4) Attention Score
= 노드와 그 이웃 노드들 사이의 상대적 중요성을 측정하는 값으로, 이 값을 통해 정보를 어떻게 통합할지 결정


GAT의 전체 학습 순서
- Input Preparation
	- Graph Structure과 Node Feature가 입력으로 주어짐
- Linear Transformation
	- Node Feature에 가중치 W를 곱함
	-- 회전 및 스케일링을 통해 Node Feature를 새로운 공간으로 매핑함
- Concatenation
	- 두 개의 노드, i와 j의 Feature Vector를 concatnate
	-- 두 노드의 feature 정보를 동시에 고려함으로써, 두 노드 사이의 관계를 더 잘 고려할 수 있음
- Weight Application
	- Concatenate된 vector에, 어텐션 스코어 계산을 위한 가중치 벡터 a를 곱함
	- 어텐션 스코어로 여러 노드 중 어떤 노드가 중요한지를 판단함
	-- a는 모델 학습 과정에서 최적화되며, 각 노드 쌍에 대해 어떤 feature가 더 중요한지 학습함 (~유사도 계산) ★이게 어떻게 되는건지?
	-- 노드 i와 노드 j의 Feature Vector가 얼마나 유사한지, 또는 얼마나 관련성이 큰지, 이걸 기준으로 중요도를 판단하나봐??
- Activation Funciton
	- Weight Application을 통해 얻은 스칼라 값에, Activation Function (ReLU 등)을 적용하여 Attention Score 산출
	- 비선형성을 도입함으로써 모델이 더 복잡한 패턴을 학습할 수 있게 됨
- Softmax Normalization
	- Attention Score에 Softmax Normalization 적용해서, 비교가 가능하도록 확률화(정규화)시킴
	-- Attention Weight = 각 이웃 노드의 정보가 현재 노드에 얼마나 영향을 주는지의 정도
- Aggregation
	- 이웃 노드들의 영향 정보인 Attention Weight를 가중평균(Weighted Sum)해서 target 노드의 Feature Vector 업데이트
	-- 이때 자기 자신의 정보도 반영하는 Self-Attention 개념도 사용됨
- Multi Head Attention
	- 병렬로 여러 개의 Attention Mechanism(=Head)을 사용하는 것
	- 그 결과들을 Concatenation이나 Average를 통해 하나로 합침
	-- Head 하나당 하나의 representation만 학습하지만, Multi-Head는 여러 개의 독립적인 Head를 사용하여 """동시에 여러 종류의 표현"""을 학습함
- Layer stacking
- Loss Calculation & Back Propagation
- Prediction & Evalution


____________________________________________________________________________________________________________
https://huggingface.co/blog/intro-graphml
// 내용이 재밌긴한데, 한국어 or 영어 원문의 문서로 공부하고 다시보는게 좋을듯, 중국어 원문이라 번역이 너무 이상하다

데이터를 사용하려면 먼저 가장 적합한 특성(homogeneous/heterogeneous, directed/undirected, and so on)을 고려해야 함



## graph level의 main tasks
- node level: 노드 속성 예측
	- (노드 속성 예측) 분자의 전체 그래프를 고려하여 원자의 3D 좌표 예측
- edge level: 에지 속성 예측 또는 누락된 에지 예측
	- (에지 속성 예측) 주어진 약물 쌍에 대한 부작용을 예측
	- (누락된 에지 예측) 추천 시스템에서 그래프의 두 노드가 관련이 있는지 예측
- sub-graph level: 커뮤니티 감지 또는 서브그래프 속성 예측
	- (커뮤니티 감지) 사람들이 어떻게 연결되어 있는지 확인
	- (서브그래프 속성 예측) 길찾기 시스템에서 예상 도착 시간을 예측


## 이러한 task를 수행하기 위한 2가지 방법이 있음
1) 특정 그래프의 변화를 예측하려는 경우 - 모든 작업(학습, 검증, 테스트)이 동일한 단일 그래프에서 수행되는 전이적 환경(transductive setting)에서의 작업
2) 대부분의 작업은 서로 다른 그래프(별도의 훈련/평가/테스트 분할)를 사용하여 수행되며, 이를 귀납적 환경(inductive setting)이라고 함


## How do we represent graphs?
- the set of all its edges
- the adjacency matrix between all its nodes

- Graph representations through ML




메시지를 집계하는 방법에 대한 주목할만한 논문들: GCN, GAT, GraphSAGE, GIN
	// 논문리뷰 ㄱ



