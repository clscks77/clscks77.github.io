---
title: "[Note] 25-02-06 ICT 강의자료.txt"
categories: [Notepad, Daily]
tags: 
date: 2025-02-06 21:49:00 +0900
comments: false
---
---

데이터 전처리와 머신러닝 모델의 성능

- 정확하게 전처리된 데이터는 모델이 데이터의 패턴을 더 잘 이해하고 학습할 수 있게 해주기 때문에 잘 처리된 데이터를 사용하여 모델을 학습시킨다면 예측 정확도가 향상되고 모델의 신뢰도가 높아짐
- 특히 시계열 데이터, 이미지 데이터, 텍스트 데이터 등 복잡한 데이터 유형에서 더욱 중요. 이러한 데이터 유형은 전처리 과정을 통해 노이즈를 제거하고, 핵심 특징을 추출하는 것이 필수적
- 또한, 데이터 전처리를 통해 모델의 과적합을 방지하고 일반화 능력을 향상시킬 수 있습니다. 이는 새로운 데이터에 대한 모델의 예측 능력을 향상시키는 역할

- 모델의 학습과 예측 성능을 극대화하기 위해서는 다양한 데이터 전처리 기술을 적절히 적용하는 것이 중요


데이터 수집 (Data Collection)
데이터 탐색 및 이해 (Exploratory Data Analysis)
결측치 처리 (Handling Missing Data)
이상치 제거 (Outlier Removal)
데이터 스케일링 (Scaling)
범주형 데이터 인코딩 (Categorical Data Encoding)
특성 선택 및 추출 (Feature Selection and Extraction)
데이터 정규화 (Data Normalization)
데이터 변환 (Data Transformation)

목표 변수(target/output variable) 가 범주형 데이터 일 때, 범주 별로 관측치의 개수, 비율의 차이가 많이 나는 데이터

(흔하다)
불균형한 데이터를 처리하는 것은 머신러닝, 
특히 한 클래스(소수 클래스)의 인스턴스 수가 다른 클래스(대수 클래스)보다 훨씬 적은 분류 작업에서 흔히 발생하는 과제

(성능 저하)
불균형한 데이터는 모델 트레이닝 및 평가에 상당한 영향을 미칠 수 있으며, 
이로 인해 편향된 모델이 과반수 클래스에 유리하고 소수 클래스에서 성능이 저하될 수 있습니다.

(다수집단의 정확성을 우선시하는 경향)
불균형한 데이터에 대해 훈련된 모델은 소수 집단을 소홀히 하면서 대다수 집단의 정확성을 우선시하는 경향
- 소수 집단 예측에서 성능 저하로 이어질 수 있음
- 정확성과 같은 지표는 불균형한 데이터 세트에서 오해의 소지가 됨
	- ex. all 1로 찍어버리기 모델
--> 정밀도, 회상, F1-score 및 ROC 곡선하 면적(AUC-ROC)과 같은 평가 지표는 정확도 단독에 비해 불균형한 데이터 세트에 더 유용함

모델을 만들 때 일반화 정도가 높은 모델을 만드는 것이 중요한데, 과적합 문제는 이러한 부분을 즉, 일반화 정도를 낮출 수 있으므로 불균형 문제를 해결하는 것이 굉장히 중요하다고 할 수 있습니다.

독립변수간의 상관관계가 매우 높을 때, 하나의 독립변수의 변화가 다른 독립변수에 영향을 미쳐, 결과적으로 모델이 크게 흔들리는 것이 다중공선정 문제
1. 무작위추출 : 무작위로 정상 데이터를 일부만 선택

2. 유의정보 : 유의한 데이터만을 남기는 방식(알고리즘 : EasyEnsemble, BalanceCascade)



데이터 희소성: 데이터가 희소해져 군집화 및 분류 작업이 어려워집니다.
계산 증가: 차원이 늘어나면 데이터 처리에 더 많은 계산 자원과 시간이 필요합니다.
과적합: 차원이 증가하면 모델이 과도하게 복잡해져 노이즈에 맞추게 됩니다. 이는 모델의 일반화 능력을 감소시킵니다.
거리의 의미 상실: 고차원에서는 데이터 포인트 간 거리 차이가 거의 없어져 유클리드 거리 같은 측정이 의미를 잃습니다.
성능 저하: k-최근접 이웃과 같은 거리 기반 알고리즘의 성능이 떨어질 수 있습니다.
시각화 어려움: 고차원 데이터는 시각화가 어려워 탐색적 데이터 분석이 어려워집니다.


특징들의 평균과 분산, 최대 최소 값이 제각각일 경우 각 특징들을 비교하기 어렵우며 학습 성능 저하로 이어짐

차원의 저주 (Curse of dimension)의 문제점에 대해 설명하고 싶어.
- 데이터 희소성 증가: 고차원 공간에서 데이터가 희소해지면 각 데이터 포인트 간의 유사성을 평가하기 어렵고, 이에 따라 군집화 및 분류 작업이 정확히 이루어지지 않을 수 있음
이런식으로 각 문제점을 한 줄로 요약해주고, 간단한 예시도 하나씩 만들어줘.
\

데이터에서 이상치는 일반적인 데이터 패턴에서 벗어나는 값이다. 이상치는 모델의 성능을 저하시킬 수 있으므로 제거 또는 대체해야 한다.
이상치를 식별하고 해당 행이나 열을 삭제하거나 다른 값으로 대체한다.


기존 변수를 조합해 새로운 변수를 만드는 기법으로, 기존 변수를 모두 사용하는 방식과 일부만 활용하는 방식이 있음
///////////////////////////////]

SMOTE, KNN 같은 방법을 사용해서 근사한 instance의 값으로 채우기
데이터 스케일링은 데이터의 크기를 조정하는 과정으로, 일반적으로는 표준화(z-score normalization) 또는 정규화(min-max scaling) 사용

소수 클래스 샘플 중 결정 경계(Decision Boundary) 근처에 있는 샘플만 선택하여 새로운 샘플을 생성하는 SMOTE 변형 기법
소수 클래스 샘플이 K-NN에서 다수 클래스에 둘러싸여 있는 경우, 해당 샘플을 중심으로 합성 데이터를 생성.

결정 경계 근처에 새로운 샘플을 추가하여 모델이 분류 성능을 향상시킬 가능성이 높음
경계 근처에서 잘못된 샘플을 생성하면 분류 성능이 오히려 저하될 수 있음


단순 복제 방식보다 다양성이 증가하여 과적합을 줄일 수 있음
소수 클래스 내부의 경계 구조를 고려하지 않으므로, 일부 샘플이 잘못 생성될 가능성이 있음.

소수 클래스 샘플과 최근접 이웃(K-NN) 사이에 새로운 합성 샘플을 생성하는 오버샘플링 기법으로, 소수 클래스 샘플의 K-NN을 찾고, 임의의 이웃과 선형 보간하여 새로운 샘플을 생성함


소수 범주에 속하는 모든 데이터에 대해, 각 데이터와 가장 가까운 K개의 데이터 중 하나를 무작위로 선정하고 Synthetic 공식을 통해 가상의 데이터를 생성


결정 경계 근처에 새로운 샘플을 추가하여 모델이 분류 성능을 향상시킬 가능성이 높음.
단점: 경계 근처에서 잘못된 샘플을 생성하면 분류 성능이 오히려 저하될 수 있음.


각 샘플의 최근접 이웃(K-NN)을 찾아 다수결로 분류된 클래스와 비교한 후, 클래스가 다르면 해당 샘플을 제거하는 방식


최근접 이웃(Nearest Neighbors, NN) 기반의 언더샘플링 기법으로, 




1. 결측치 처리 (Handling Missing Data)
- 데이터에 NaN(Not a Number) 또는 Null, None 등의 값으로 표시되는 결측치가 존재하는 경우
- 평균값, 중앙값, 최빈값 등으로 결측치를 채우거나 삭제하여 데이터의 완전성을 높임
   - SMOTE, KNN 같은 방법을 사용해서 근사한 instance의 값으로 채우기

2. 이상치 제거 (Outlier Removal)
- 

3. 범주형 데이터 인코딩 (Categorical Data Encoding)
- 







