---
title: "[Note] 25-02-15 ICT 10주차.txt"
categories: [Notepad, Daily]
tags: 
date: 2025-02-15 21:49:00 +0900
comments: false
---
---


나이브 베이즈
- 나이브하지 않은 베이지언 분류? 현실성이 없다?
- 나이브한 해법
- 수치형 예측 변수

판별분석
- 공분산행렬
- 피셔의 선형판별

로지스틱 회귀
- 함수, 로짓
- LR와 GLM
- 일반화선형모형
- LR의 예측값
- 계수와 오즈비 해석하기?
- 선형회귀와 로지스틱 회귀 비교
- 모델 평가하기~

분류 모델 평가하기
- 혼동 행렬
- 희귀 클래스 문제
- 정밀도, 재현율, 특이도
- ROP 곡선
- AUC
- 리프트?

Pros

모든 독립변수가 독립적인 관계일 것이라는 나이브한 대전제에도 불구하고 실전에서 꽤 높은 성능을 보이며 문서 분류 문제의 경우 특히나 강한 면모를 보인다.
Cons

비교적 많은 경우에 성능이 괜찮을뿐이지 독립의 가정을 하기 어려운 데이터들이나 다른 유형의 분류 문제에서는 적용하기 어렵다.
단어 빈도와 같이 이산형 독립변수에 대해서는 이전에 관측되지 않았던 데이터라 우도가 0이 되어버리는 경우가 발생할 위험이 있다. 이에 스무딩 이라는 방법을 활용하여 정확하지 않은 zero-likelihood 문제를 해결할 수 있다.


1.
설명변수가 연속형 변수일 때, Gaussian Naive Bayes (가우시안 나이브 베이즈)

2.
설명변수가 범주형 변수일 때, Multinomial Naive Bayes (다항 나이브 베이즈) 

3.
보완적인 나이브 베이즈 (Complement Naive Bayes)
이건 뭔ㅁ데


4.
같은 범주형 변수일 때, 범주가 2개밖에 없는 이진형일 경우 Bernoulli naive Bayes (베르누이 나이브 베이즈)로 분류

다변량 베르누이 분포 형식을 따라하는 데이터를 학습하고 분류하기 위한 최적의 알고리즘이다. 
주로, 이진값을 가진 것에 대하여 분류를 하고 이러한 요소는 베르누이를 적용하기에 아주 특화되어 있다. 
아래의 수학 공식을 보면 2가지 x, 1-x로 나눠줘 있어서 서로 다르게 분류하기가 괜찮다.



(샘플별 확률 추정)
나이브 베이즈 알고리즘으로 주어진 샘플이 어떤 클래스에 속할 확률을 추정
- 이때 확률 계산을 위한 분포를 가정하는데, 종류가 여러가지 있음

(클래스 예측)
알려진 모든 클래스 y_i에 대해 확률 p(y=y_i | x)를 계산하고 확률이 가장 높은 클래스를 반환



(매개변수 학습)
모든 클래스에 대해 샘플이 특정 클래스에 속할 확률을 비교하여, 입력 샘플에 대한 예측 클래스를 반환하는 도구인 분류기 생성
- 모델 학습을 통해 분류기의 가우시안 분포 매개변수를 추정




##
회귀란?



##
Linear Regression

제일제일 간단한 모델
basis function이라고 부를 것이다. x의 기저를 바꾸는 행위
polynomial(다항식) basis를 사용한다고 하자. 그러면 아래의 그림과 같이 포물선 형태의 모델을 얻을 수 있을 것이다.
기저함수와 weight의 선형조합으로 target값을 예측 = linear regression


##
다중선형회귀(Multiple Linear Regression)는 
수치형 설명변수 X와 연속형 숫자로 이뤄진 종속변수 Y 간의 관계를 선형으로 가정하고, 이를 가장 잘 표현할 수 있는 회귀계수를 데이터로부터 추정하는 모델

Linear Regression은 연속형 종속 변수와 연속형 독립 변수 간의 관계를 모델링


회귀계수들은 모델의 예측값과 실제값의 차이, 즉 오차제곱합(error sum of squares)을 최소로 하는 값들

33명의 성인 여성에 대한 나이와 혈압 데이터 예시


그러면 혈압이라는 연속형 숫자 대신 범주형 변수를 이용해 위와 같은 회귀모델을 구축한다면?


위와 같은 문제가 발생하는 근본적인 이유는 종속변수 Y의 성질 때문

혈압의 경우 숫자 그 자체로 의미를 지니는 변수이지만, 암 발생여부는 그렇지 않습니다. 
정상을 1, 발병을 0으로 바꾸어도 큰 상관이 없습니다. 
숫자가 아무 의미를 지니지 않는다는 얘기

이처럼 Y가 범주형(categorical) 변수일 때는 다중선형회귀 모델을 그대로 적용할 수 없다는 겁니다. 이러한 문제 때문에 로지스틱 회귀 모델이 제안됐습니다.★


## 
로지스틱 함수(Logistic Function)와 승산(Odds)

실제 많은 자연, 사회현상에서는 특정 변수에 대한 확률값이 선형이 아닌 S-커브 형태를 따르는 경우가 많다
이러한 S-커브를 함수로 표현해낸 것이 로지스틱 함수 = 시그모이드 함수

x 값으로 어떤 값이든 받을 수가 있지만 출력 결과는 항상 0에서 1사이 값인
확률밀도함수(probability density function) 요건을 충족시키는 함수


승산(Odds)이란 임의의 사건 A가 발생하지 않을 확률 대비 일어날 확률의 비율 (=실패[0]에 대한 성공[1] 비율)

로짓(Logit): -∞ ~ ∞ 범위에서 어떤 클래스에 속할 확률을 결정하는 함수 
= 로그 오즈(log odds): 변환모델의 응답변수. 이 값을 통해 확률을 구한다.




문제 = 종속변수 Y가 연속형 숫자가 아닌 범주일 때는 기존 회귀 모델을 적용할 수 없
해결 = Y를 범주1이 될 확률로 두고 식을 세워 보자

(회귀식은 -∞ ~ ∞ 이므로 이에 맞춰 Y를 변형시켜야 함)
Y를 단순히 확률로만 바꾸면 0~1이 됨
이를 odd로 바꾸면 0~∞이 됨
여기에 log를 취하면  -∞ ~ ∞ 범위 완성
(로그승산)

이를 입력벡터 x가 주어졌을 때 범주1일 확률을 기준으로 정리하면
로지스틱 함수의 꼴이 됨
--> 입력벡터 x를 넣으면 범주 1에 속할 확률을 반환해 줌

그럼 그 확률값이 얼마나 되어야 범주 1로 분류할 수 있을까요?



βTx<0
  이면 해당 데이터의 범주를 0으로 분류하게 됩니다. 따라서 로지스틱모델의 결정경계(decision boundry)는 βTx=0
인 하이퍼플레인(hyperplane



출력변수 함수로 로지스틱 함수를 사용함으로써 Range(치역)를 [0, 1]로
로지스틱 함수를 로지스틱 회귀 모델의 출력변수 함수로 사용하여 0부터 1사이의 값을 리턴?




로지스틱 회귀는 독립 변수와 종속 변수 간의 관계를 모델링하는 데 사용되며, 결과는 
종속 변수의 확률값으로 나타냅니다.




##
K 개 범주를 분류하는 다항로지스틱 회귀 모델




다중 회귀 식을 이용해서 y 값을 구하고 그 값을 시그모이드 함수(또는 로지스틱 함수)를 통과시켜서 값의 범위를 줄여서 확률로 만들고 그 값을 통해 라벨을 예측










이항로지스틱 회귀 모델 K−1개를 써서 K−1개 회귀계수 벡터가 도출


로지스틱 회귀 모델은 독립 변수의 계수(즉, 기울기 매개 변수)를 추정
이 계수는 해당 독립 변수의 한 단위 변화에 대한 종속 변수의 로그 오즈 변화를 나타내며,
다른 모든 변수를 일정하게 유지하는 동안 적용됩니다.


##
로지스틱회귀 모델 계수를 추정
== Model Fitting (모델 피팅): 로지스틱 함수에서 β0,β1 값을 찾아내는 작업

 대표적인 모델 피팅 방법 중 하나   MLE(Maximum Likelihood Estimation)
★최대우도추정법(Maximum Likelihood Estimation)  

확률을 최대로 만들어주는 Parameter값 찾기




##
로그 손실
로지스틱 회귀에서 제곱 손실이 아닌 로그 손실을 사용하는 이유를 설명합니다.



##
정규화
로지스틱 회귀 모델 학습 시 정규화의 중요성을 설명합니다.






##
로지스틱 회귀 분석의 응용 분야




로지스틱 회귀 모델은 일반화 선형 모델의 일종으로,






