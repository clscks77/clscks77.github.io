---
title: "[Note] 2024-12-31 xai.txt"
categories: [Notepad, Daily]
tags: 
date: 2024-12-31 21:49:00 +0900
comments: false
---
---


## KEYWORD
- Cyber Forensics (= Digital Forensics): "법적으로 허용되는 방식으로" 디지털 증거를 식별, 보존, 분석 및 제시하는 관행
- Blockchain
- 불법거래 탐지, 사기 탐지
- XAI		// AI로 넘아가면 "적대적 공격에 대한 회복성"을 다음 flow로 해야 함
- GNN


## GOAL
- 블록체인 포렌식과 같은 고위험 분야에 AI를 책임감 있게 적용하는 방법
	- 포렌식 도메인을 특정한다면? => 블록체인 포렌식 (블록체인 거래, 스마트 계약 분석)
- AI 의사결정 프로세스의 투명성을 높이고 법의학 조사자가 활용할 수 있도록 보장

_______________________________________________________________________________________________________________________________________


1.
XAI-CF -- Examining the Role of Explainable Artificial Intelligence in Cyber Forensics [arxiv '24]




2. 내가 하고싶은걸 그대로 하신 분이 계심
Large Language Model XAI approach for illicit activity Investigation in Bitcoin [Neural Computing and Applications '24]	// 이걸 6만원 주고 읽어야 함ㅁ
-- 같은 사람이네 (Jack Nicholls)-- Enhancing Illicit Activity Detection using XAI: A Multimodal Graph-LLM Framework [arxiv '23]
-- 다 이사람이었음-- SoK: The Next Phase of Identifying Illicit Activity in Bitcoin
		-- FraudLens: Graph Structural Learning for Bitcoin Illicit Activity Identification
	// https://www.linkedin.com/in/jack--nicholls/




3. XAI section in NIPS 2023
https://neurips.cc/virtual/2023/workshop/66529





_______________________________________________________________________________________________________________________________________
##1. XAI-CF -- Examining the Role of Explainable Artificial Intelligence in Cyber Forensics [arxiv '24]


Abstract
- CF에 AI를 성공적으로 적용하려면 AI 시스템에 대한 신뢰를 구축해야 합니다
- 설명 가능한 AI(XAI) 시스템은 CF에서 이러한 역할을 할 수 있으며, 이러한 시스템을 XAI-CF라고 합니다.
- 우리는 성공적이고 실용적인 XAI-CF 시스템을 구축해야 할 필요성을 강조하고, 그러한 시스템의 주요 요구 사항과 전제 조건 중 일부를 논의합니다. ★

1. Introduction and Motivation
- (CF의 과제) CF의 과제에 대한 자세한 목록과 논의는 [3, 4]에서 확인할 수 있습니다. 
- (AI의 과제) AI는 강력한 기술이며 잠재적인 이점을 가지고 있지만, 특히 딥러닝을 사용할 때 
	본질적으로 설명할 수 없는 투명성 부족, 적대적 공격, 개인정보, 잘못된 정보, 알고리즘 및 데이터 편향, 윤리 등의 잠재적인 위험과 과제를 수반합니다. 
- (CF에서의 AI 활용 현황) 인공지능은 사이버 보안뿐만 아니라 다양한 분야에서 성공적으로 적용되고 있지만,
	특히 CF에서의 적용과 활용은 아직 상대적으로 제한적입니다 [13]. (2021)
- (Digital Forensic AI 표준) 이 격차를 해소하기 위해서는 CF에서 AI의 잠재적 효과를 조사 및 탐색하고 사이버 증거를 채굴하기 위한 표준화된 기술을 개발해야[14]
[14] A. Solanke, M. Biasiotti, Digital forensics ai: evaluating, standardizing and optimizing digital evidence mining techniques, Ki- K¨ unstliche Intelligenz 36 (2022) 

- (Lack of Transparency) AI 시스템의 투명성 부족은 CF 분야에서 다양한 과제를 야기합니다. 주요 과제 중 하나는 인간 사용자의 검증이 필요하다는 점입니다[16]. 
[16] S. W. Hall, A. Sakzad, K. R. Choo, Explainable artificial intelligence for digital forensics, WIREs Forensic Science 4 (2021).
	- 법정에서 AI를 사용한 CF 분석은 종종 법적 기준에 따라 정당화하거나 설명할 수 없는 블랙박스로 간주되기 때문에 인정받기 어렵다 [15]


[17] N. Capuano, G. Fenza, V. Loia, C. Stanzione, Explainable artificial intelligence in cybersecurity: a survey, IEEE Access 10 (2022) 93575–93600.



4. Background
4.1. Explainable Artificial Intelligence (XAI)

(1) Using simple models
   - such as decision Tree and other rule-based approaches
(2) Feature selection and analysis
   - focusing on a small set of features
(3) uses external mechanisms to help describe the inner workings of a black-box AI system.
   - creates a proxy model
   - perturbs the model by querying the model with different inputs to assess which inputs are important.
	- LIME (Local Interpretable Model-Agnostic Explanations) [37]
	- SHAP (SHapley Additive exPlanation) [38, 39]


National Institute of Standards and Technology [NIST] introduced four principles [40] and proposed that all XAI systems should adhere to these principles.

1. Explanation – 결과를 논리적으로 설명할 수 있는가?
	Self-Explainable -> 의사 결정 트리와 회귀 모델
	Post-Hoc Explainable ->  다른 소프트웨어 도구에 의해 렌더링
	- feature 중요도 점수, 규칙 세트, 히트맵 또는 자연어 표현을 생성함
2. Meaningful – 의미있는 설명인가?
	XAI 시스템은 맥락에 적합하고 의도한 사용자가 이해할 수 있는 의미 있는 정보를 제공해야 함
3. Explanation Accuracy – 결론에 도달하는 과정을 정확하게 설명하기 --> AI 알고리즘 및 시스템의 정확도와는 다름. 
	설명의 세부 수준 포함
4. Knowledge Limits – 지식의 한계를 모델이 인정하는가?
	잘모르겠는 애매한거를 막 던지지 않고 잘모르겠다 하는거(인듯)


4.1.1. Classification of XAI Methods
   1. Local Explanations
	- LIME [37] (2016)
	- SHAP [38, 39] (2017, 2020)
	- Saliency Map [66, 67] (2017, 2018)
	- Counterfactual [68] (2017)
   2. Global Explanations
	- PDP (Partial Dependence Plot) [69] (2001)	// 넘 옛날인디
	- ICE (Individual Conditional Expectation) [70] (2015)
	- GSA (Global Sensitivity Analysis) [71, 72] (2011, 2013)
	- SP-LIME (Submodular Pick LIME) [37] (2016)




 CF가 이를 최대한 활용하고 직면한 과제[4, 128]




