---
title: "[Note] 25-04-03 AML_LLM"
categories: [Notepad, Daily]
tags: [research, aml, llm]
date: 2025-04-03 21:49:00 +0900
comments: false
excerpt: "교수님이 LLM 관점의 idea를 만들어보라고 하셨다. AML, Graph, Blockchain과 관련된 LLM 연구를 찾아보자."
---
---

## LLM & AML

### Revolutionizing cyber threat detection with large language models [arXiv, 2023]
- Natural Language Processing (NLP) domain is experiencing a revolution due to the capabilities of Pre-trained Large Language Models ( LLMs), fueled by ground-breaking Transformers architecture, resulting into unprecedented advancements. Their exceptional aptitude for assessing probability distributions of text sequences is the primary catalyst for outstanding improvement of both the precision and efficiency of NLP models. This paper introduces for the first time SecurityLLM, a pre-trained language model designed for cybersecurity threats detection. The SecurityLLM model is articulated around two key generative elements: SecurityBERT and FalconLLM. SecurityBERT operates as a cyber threat detection mechanism, while FalconLLM is an incident response and recovery system. To the best of our knowledge, SecurityBERT represents the inaugural application of BERT in cyber threat detection. Despite the unique nature of the input data and features, such as the reduced significance of syntactic structures in content classification, the suitability of BERT for this duty demonstrates unexpected potential, thanks to our pioneering study. We reveal that a simple classification model, created from scratch, and consolidated with LLMs, exceeds the performance of established traditional Machine Learning (ML) and Deep Learning (DL) methods in cyber threat detection, like Convolutional Neural Networks (CNN) or Recurrent Neural Networks (RNN). The experimental analysis, conducted using a collected cybersecurity dataset, proves that our SecurityLLM model can identify fourteen (14) different types of attacks with an overall accuracy of 98%.


### Investlm: a large language model for investment using financial domain instruction tuning [arXiv, 2023]
- We present a new financial domain large language model, InvestLM, tuned on LLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset related to financial investment. Inspired by less-is-more-for-alignment (Zhou et al., 2023), we manually curate a small yet diverse instruction dataset, covering a wide range of financial related topics, from Chartered Financial Analyst (CFA) exam questions to SEC filings to Stackexchange quantitative finance discussions. InvestLM shows strong capabilities in understanding financial text and provides helpful responses to investment related questions. Financial experts, including hedge fund managers and research analysts, rate InvestLM's response as comparable to those of state-of-the-art commercial models (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of financial NLP benchmarks demonstrates strong generalizability. From a research perspective, this work suggests that a high-quality domain specific LLM can be tuned using a small set of carefully curated instructions on a well-trained foundation model, which is consistent with the Superficial Alignment Hypothesis (Zhou et al., 2023). From a practical perspective, this work develops a state-of-the-art financial domain LLM with superior capability in understanding financial texts and providing helpful investment advice, potentially enhancing the work efficiency of financial professionals. We release the model parameters to the research community.


### Enhancing Illicit Activity Detection using XAI: A Multimodal Graph-LLM Framework [arXiv, 2023]
- Financial cybercrime prevention is an increasing issue with many organisations and governments. As deep learning models have progressed to identify illicit activity on various financial and social networks, the explainability behind the model decisions has been lacklustre with the investigative analyst at the heart of any deep learning platform. In our paper, we present a state-of-the-art, novel multimodal proactive approach to addressing XAI in financial cybercrime detection. We leverage a triad of deep learning models designed to distill essential representations from transaction sequencing, subgraph connectivity, and narrative generation to significantly streamline the analyst's investigative process. Our narrative generation proposal leverages LLM to ingest transaction details and output contextual narrative for an analyst to understand a transaction and its metadata much further.


### Prompting large language models for malicious webpage detection [PRML, 2023]
- This work proposes a novel approach for malicious webpage detection by leveraging Large Language Models (LLMs). Unlike existing approaches that only analyze the Uniform Resource Locators (URLs) features, our approach considers the web contents for identifying malicious webpages. The major challenge is the lack of large-scale malicious analysis datasets with crawled web content for training previous data-driven models. To mitigate the challenge, we investigate prompting LLMs for the malicious webpage detection task, thus breaking the constraint of annotated training data. By using the popular GPT-3.5 and ChatGPT as our LLM engines, we study zero-shot and few-shot prompting methods to adapt those LLMs to perform malicious webpage detection. Experimental results show that our proposed approach achieves comparable or even better performance than deep learning baselines. Our analysis highlights the importance of integrating webpage content in detecting malicious URLs and demonstrates the feasibility of using LLMs to detect cybersecurity threats.


### Leveraging XAI in Prompt-Based ChatGPT for Financial Decision Support [DEBAI, 2024]
- Neural network models have demonstrated good results in credit evaluation and related fields. However, they are characterized as "black boxes" due to their lack of interpretability. To enhance transparency and credibility, various Explainable Artificial Intelligence (XAI) methods have been proposed. Yet, the raw outputs provided by these XAI methods are still challenging for non-experts to understand. This study presents an innovative approach that applies Large Language Models (LLMs) to credit evaluations. We design contextual prompts that integrate XAI raw outputs into the instructions for LLMs, which then generate domain-specific, and non-expert-friendly explanations in natural language. Experimental results show that the proposed method is capable of generating straightforward explanations of machine learning models for users in different roles.


### Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin Case Study [arXiv, 2025]
- Cryptocurrencies are widely used, yet current methods for analyzing transactions heavily rely on opaque, black-box models. These lack interpretability and adaptability, failing to effectively capture behavioral patterns. Many researchers, including us, believe that Large Language Models (LLMs) could bridge this gap due to their robust reasoning abilities for complex tasks. In this paper, we test this hypothesis by applying LLMs to real-world cryptocurrency transaction graphs, specifically within the Bitcoin network. We introduce a three-tiered framework to assess LLM capabilities: foundational metrics, characteristic overview, and contextual interpretation. This includes a new, human-readable graph representation format, LLM4TG, and a connectivity-enhanced sampling algorithm, CETraS, which simplifies larger transaction graphs. Experimental results show that LLMs excel at foundational metrics and offer detailed characteristic overviews. Their effectiveness in contextual interpretation suggests they can provide useful explanations of transaction behaviors, even with limited labeled data.


================================================================================

- **LLM이 에이전트의 “자금 세탁 전략”을 계획하도록 하기**
    - 즉, 에이전트가 매현재 상태(잔고, 주소 수, 위험도, 사용 가능한 체인, 컨트랙트 등)를 기반으로 LLM에게 프롬프트를 주고,  
    LLM이 합리적인 세탁 시퀀스를 생성함
        - 비현실적 응답 제거 필요

- **Wallet-to-Entity Mapping (지갑 클러스터링)**
    - LLM을 “온체인 트랜잭션 히스토리 또는 메타데이터 해석기”로 활용
    - GNN 등 탐지기에 추가 feature로 활용 가능

================================================================================

## LLM & Graph

### Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs [SIGKDD 2024]
- Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at: https://github.com/CurryTang/Graph-LLM .

---

## GNN & AML

### Anti-money laundering in bitcoin: experimenting with graph convolutional networks for financial forensics. [arXiv, 2019]

---

## Bitcoin & AML

### Revealing and concealing bitcoin identities: a survey of techniques [BSCI, 2021]