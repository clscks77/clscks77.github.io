---
title: "[Gen] Prompt-to-Prompt Image Editing with Cross-Attention Control [ICLR, 2023]"
categories: [PaperReview, GenerativeAI]
tags: [vision, diffusion, image-editing]
date: 2025-05-04 18:57:12 +0900
comments: false
excerpt: "ë¹„ì „ 12ì£¼ì°¨-Text-guided Image Editing ë…¼ë¬¸ ë¦¬ë·°"
--- 
---

- paper: <https://arxiv.org/abs/2208.01626>
- github: <https://github.com/google/prompt-to-prompt/>
- review ref: <https://jang-inspiration.com/prompt-to-prompt-image-editing-with-cross-attention-control>
- open review(å¿…è®€): <https://openreview.net/forum?id=_CDixzkzeyb>

```
To this end: ì´ë¥¼ ìœ„í•´
phenomenal: ê²½ì´ì ì¸
amplify or attenuate: ì¦í­ ë˜ëŠ” ê°ì‡ 
encompassing: í¬í•¨í•˜ë‹¤, ë§ë¼í•˜ë‹¤, ì•„ìš°ë¥´ë‹¤
auxiliary input: ë³´ì¡° ì¸í’‹
in tandem with CLIP: CLIPê³¼ í•¨ê»˜ (ë‚˜ë€íˆ, í˜‘ë ¥í•˜ì—¬)
Seminal works: ì¤‘ëŒ€í•œ ì—°êµ¬ (ì˜í–¥ë ¥ì´ í°)
revolutionary: íšê¸°ì ì¸, í˜ëª…ì ì¸, í˜ì‹ ì ì¸
curated: ì „ë¬¸ì ì¸ ì‹ê²¬ìœ¼ë¡œ ì—„ì„ í•œ
struggle over large datasets: í° ë°ì´í„°ì…‹ì—ì„œëŠ” ì–´ë ¤ì›€ì„ ê²ªìŒ 
```


---

> í•µì‹¬ ê°œë…ì€ cross-attention map ì¡°ì‘ì„ í†µí•´ diffusion ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ì„¸ë°€í•˜ê²Œ ì œì–´í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ


## Abstract

large-scale text-driven synthesis models
- ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ë§¤ìš° ë‹¤ì–‘í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸
- ì´ë¥¼ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ë¯¸ì§€ **í¸ì§‘**ìœ¼ë¡œ í™•ì¥í•˜ê³ ì í•˜ëŠ” ì—°êµ¬
> ê¸°ì¡´ì˜ ëŒ€ì„¸ ì—°êµ¬ë¥¼ í™•ì¥í•˜ê³ ì í•œë‹¤

generative models
- ì›ë³¸ ì´ë¯¸ì§€ì˜ ëŒ€ë¶€ë¶„ì„ ë³´ì¡´í•˜ëŠ” ê²ƒì´ í¸ì§‘ ê¸°ìˆ ì˜ ë³¸ì§ˆì ì¸ íŠ¹ì„±
text-based models
- í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ê¸ˆë§Œ ìˆ˜ì •í•´ë„ ì™„ì „íˆ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— í¸ì§‘ì´ ê¹Œë‹¤ë¡œì›€
> ì´ í¸ì§‘ ì—°êµ¬ê°€ ê¹Œë‹¤ë¡œìš´ ì´ìœ ê°€ ìˆë‹¤

State-of-the-art methods 
- ì‚¬ìš©ìê°€ ê³µê°„ ë§ˆìŠ¤í¬ë¥¼ ì œê³µí•˜ì—¬ í¸ì§‘ ìœ„ì¹˜ë¥¼ ì§€ì •í•˜ë„ë¡ ìš”êµ¬í•¨ìœ¼ë¡œì¨ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ì™„í™”í•˜ê³ , ë”°ë¼ì„œ ë§ˆìŠ¤í¬ëœ ì˜ì—­ ë‚´ì˜ ì›ë³¸ êµ¬ì¡°ì™€ ì½˜í…ì¸ ë¥¼ ë¬´ì‹œ
> í˜„ì¬ ì˜íƒ€ëŠ” ì‚¬ìš©ìê°€ ê³µê°„ ë§ˆìŠ¤í¬ë¥¼ ì§€ì •í•˜ê³ , ì´ë¥¼ ë¬´ì‹œí•˜ë„ë¡ í–ˆë‹¤.

In this paper
- ì €ìëŠ” í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ í¸ì§‘ì„ ì œì–´í•˜ëŠ” ì§ê´€ì ì¸ Prompt-to-Prompt editing frameworkë¥¼ ê³ ì§‘í•¨
- ì´ë¥¼ ìœ„í•´(To this end)
    - text-conditioned modelì„ ì‹¬ì¸µ ë¶„ì„í•˜ê³ ,
    - **cross-attention layer**ê°€ ì´ë¯¸ì§€ì˜ spatial layoutê³¼ promptì˜ ê° ë‹¨ì–´ ê°„ì˜ **ê´€ê³„ë¥¼ ì œì–´í•˜ëŠ” í•µì‹¬ ìš”ì†Œì„**ì„ ê´€ì°°í•¨
    > ë¶„ì„ê³¼ ê´€ì°°ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ insight ë©‹ì§€ë‹¤. ì´ê²Œ academicì´ì§€
- ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ textual promptë§Œ í¸ì§‘í•˜ì—¬ ì´ë¯¸ì§€ í•©ì„±ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ëª‡ ê°€ì§€ applicationì„ ë³´ì—¬ì¤Œ
    - localized editing by replacing a word, global editing by adding a speciï¬cation, and even delicately controlling the extent(ì •ë„) to which a word is reï¬‚ected in the image.



---

## 1 Introduction

- **large-scale language-image (LLI) models (ëŒ€ê·œëª¨ ì–¸ì–´ ì´ë¯¸ì§€ ëª¨ë¸)**
    - Imagen [38], DALLÂ·E 2 [33] and Parti [48]
    - ë§¤ìš° í° language-image ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµë¨
    - **auto-regressive model**ê³¼ **diffusion model**ì„ í¬í•¨í•œ SOTA ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì„ ì‚¬ìš©
        > â—† auto-regressive modelì€ ë­ì§€?
- (í•œê³„) í¸ì§‘ ìˆ˜ë‹¨ì„ ì œê³µí•˜ì§€ ì•Šìœ¼ë©°, ì´ë¯¸ì§€ì˜ íŠ¹ì • semantic ì˜ì—­ì— ëŒ€í•œ ì œì–´ê°€ ë¶€ì¡±í•¨
    - íŠ¹íˆ, í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì— ì•½ê°„ì˜ ë³€í™”ë§Œ ìˆì–´ë„ ì™„ì „íˆ ë‹¤ë¥¸ ì¶œë ¥ ì´ë¯¸ì§€ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆë‹¤.

- **(ê¸°ì¡´ ì—°êµ¬) LLI-based methods [28, 4, 33]**
    > [28] Alex Nichol, et. el. Glide: Towards photorealistic image generation and editing with text-guided diffusion models [arXiv, 2021] [ICML, 2022]  
	> [4] Omri Avrahami, et. el. Blended latent diffusion [arXiv, 2022] [ACM TOG, 2023]  
	> [33] Aditya Ramesh, et. el. Hierarchical text-conditional image generation with clip latents [arXiv, 2022] (= OpenAIì˜ DALLÂ·E 2ë…¼ë¬¸)
    - ì‚¬ìš©ìê°€ ì¸í˜ì¸íŒ…(inpainting)í•  ì´ë¯¸ì§€ì˜ ì¼ë¶€ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë§ˆìŠ¤í‚¹í•˜ê³ , ì›ë˜ ì´ë¯¸ì§€ì˜ ë°°ê²½ê³¼ ì¼ì¹˜ì‹œí‚¤ë©´ì„œ ë§ˆìŠ¤í‚¹ëœ ì˜ì—­ì—ì„œë§Œ ë³€ê²½ë˜ë„ë¡ í•¨
        - ì´ ë°©ì‹ì´ ê²°ê³¼ëŠ” ì¢‹ì•˜ì§€ë§Œ, **ë§ˆìŠ¤í‚¹ ì ˆì°¨ê°€ ë²ˆê±°ë¡­ê³ , ë¹ ë¥´ê³  ì§ê´€ì ì¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ í¸ì§‘ì„ ë°©í•´**í•¨
        - ë˜í•œ, ì´ë¯¸ì§€ ì½˜í…ì¸ ë¥¼ ë§ˆìŠ¤í‚¹í•˜ë©´ **ì¤‘ìš”í•œ structural informationê°€ ì œê±°**ë˜ë©°, ì´ëŠ” ì¸í˜ì¸íŒ… í”„ë¡œì„¸ìŠ¤ì—ì„œ ì™„ì „íˆ ë¬´ì‹œë¨
    - ë”°ë¼ì„œ íŠ¹ì • ê°œì²´ì˜ texture(í…ìŠ¤ì²˜) ìˆ˜ì •ê³¼ ê°™ì€ ì¼ë¶€ í¸ì§‘ ëŠ¥ë ¥ì€ ì¸í˜ì¸íŒ… ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨
        > â—† ???? ì™œ???? structural informationì´ ì—†ì–´ì ¸ì„œ "íŠ¹ì • ê°œì²´" ì´ê²Œ ì•ˆë˜ëŠ”ê±´ê°€?

- (ë³¸ ë…¼ë¬¸ì˜ ë°©ë²•) Prompt-to-Prompt ì¡°ì‘ì„ í†µí•´ **pre-trained ëœ text-conditioned diffusion modelì—ì„œ?**
    - ì´ë¯¸ì§€ë¥¼ **ì˜ë¯¸ë¡ ì ìœ¼ë¡œ í¸ì§‘í•˜ëŠ”** ì§ê´€ì ì´ê³  ê°•ë ¥í•œ í…ìŠ¤íŠ¸ í¸ì§‘(textual editing) ë°©ë²•
    > pre-trained ë¶€ë¶„ì´ ê¶ê¸ˆí•¨.  
    > text-conditioned diffusion modelì€ ê¸°ì¡´ì— ìˆë˜ê±´ê°€?
    
- ì´ë¥¼ ìœ„í•´ **cross-attention layer**ë¥¼ ê¹Šì´ íŒŒê³ ë“¤ê³ , ì´ë¯¸ì§€ ìƒì„±ì„ ì œì–´í•˜ëŠ” handleë¡œì„œì˜ **ì˜ë¯¸ë¡ ì  ê°•ì (semantic strength)**ì„ íƒêµ¬
    - íŠ¹íˆ, prompt textì—ì„œ ì¶”ì¶œí•œ pixelê³¼ tokenì„ ê²°í•©í•˜ëŠ” **ê³ ì°¨ì› í…ì„œì¸ ë‚´ë¶€ cross-attention map**ì„ ê³ ë ¤í–ˆê³ ,
    - ì €ìë“¤ì€ ì´ëŸ¬í•œ ë§µì´ ìƒì„±ëœ ì´ë¯¸ì§€ì— ê²°ì •ì ìœ¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” **í’ë¶€í•œ ì˜ë¯¸ë¡ ì  ê´€ê³„ë¥¼ í¬í•¨**í•˜ê³  ìˆìŒì„ ë°œê²¬í•¨
    > íƒêµ¬ ê²°ê³¼: cross-attention layer **ë‚´ë¶€ cross-attention map**ì´ ì¤‘ìš”í•œ **ì˜ë¯¸ë¡ ì  ê´€ê³„**ë¥¼ í¬í•¨í•˜ê³  ìˆì—ˆê³ , ì´ê²Œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±ì˜ í•µì‹¬ì´ë”ë¼

- (í•µì‹¬ ì•„ì´ë””ì–´) diffusion process ì¤‘ì— cross-attention mapì„ ì£¼ì…í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í¸ì§‘í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ
    - ì¦‰, ê° Diffusion step ì¤‘ì— ì–´ë–¤ í”½ì…€ì´ prompt textì˜ ì–´ë–¤ í† í°ì— attention ë˜ëŠ”ì§€ ì œì–´í•œë‹¤ëŠ” ê²ƒ
    > ê·¸ë ‡ë‹¤ë©´, cross-attention mapì„ Diffusion ê³¼ì • ì¤‘ì— ë”°ë¡œ ì£¼ì…í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í¸ì§‘ì„ ì œì–´í•´ë³´ì? ì´ê±´ê°€?

- ë³¸ ë…¼ë¬¸ì˜ ë°©ë²•ì„ ë‹¤ì–‘í•œ ì°½ì˜ì  í¸ì§‘ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì ìš©í•˜ê¸° ìœ„í•´ ê°„ë‹¨í•˜ê³  ì˜ë¯¸ë¡ ì ì¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ cross-attention mapì„ ì œì–´í•˜ëŠ” ëª‡ ê°€ì§€ ë°©ë²•ì„ ë³´ì—¬ì¤€ë‹¤.
    1. **promptì—ì„œ ë‹¨ì¼ token ê°’ì„ ë³€ê²½(ex.ê°œ â†’ ê³ ì–‘ì´):** scene composition(ì¥ë©´ êµ¬ì„±)ì„ ìœ ì§€í•˜ë©´ì„œ cross-attention maps ìˆ˜ì •
    2. **ì „ì²´ì ìœ¼ë¡œ í¸ì§‘(globally edit; ìŠ¤íƒ€ì¼ ë³€ê²½):** promptì— ìƒˆë¡œìš´ wordë¥¼ ì¶”ê°€í•˜ëŠ”ë°, ì´ì „ tokenì— ëŒ€í•œ attentionì€ ê³ ì •í•˜ë©´ì„œ, ìƒˆ tokenìœ¼ë¡œ ìƒˆë¡œìš´ attentionë¥¼ ì´ë™ì‹œí‚¬ ìˆ˜ ìˆê²Œ í•¨
    3. **wordì˜ ì˜ë¯¸ë¡ ì  íš¨ê³¼ë¥¼ ì¦í­í•˜ê±°ë‚˜ ì•½í™”ì‹œí‚´(amplify or attenuate)**

- ë³¸ ë…¼ë¬¸ì˜ ì ‘ê·¼ ë°©ì‹ì€ textual promptë§Œ í¸ì§‘í•˜ì—¬ **ì§ê´€ì ì¸ ì´ë¯¸ì§€ í¸ì§‘ ì¸í„°í˜ì´ìŠ¤**ë¥¼ êµ¬ì„±í•˜ëŠ” ê²ƒ = Prompt-to-Promptë¼ê³  ë¶€ë¥¸ë‹¤.
    - model training, finetuning, ì¶”ê°€ ë°ì´í„° ë˜ëŠ” ìµœì í™”ê°€ í•„ìš”í•˜ì§€ ì•Šë‹¤.
        > ê·¸ëŸ¼ ì–´ë–»ê²Œ?? ìƒë‹¹íˆ í¥ë¯¸ë¡œìš´ ë¶€ë¶„â—†
    - ì €ìë“¤ì˜ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ë³´ë©´, edited prompt ì™€ source image ì‚¬ì´ì˜ ï¬delityì— ëŒ€í•œ trade-offë¥¼ ìµì‹í•¨ìœ¼ë¡œì¨, ì´ë¯¸ì§€ ìƒì„± ì œì–´ë¥¼ ë” í•  ìˆ˜ ìˆë‹¤?
        > â—† edited prompt ì™€ source image ì‚¬ì´ì˜ ï¬delityì— ëŒ€í•œ trade-offë¥¼ ìµì‹í•œë‹¤?
    - ì €ìë“¤ì€ ê¸°ì¡´ì˜ inversion processë¥¼ ì‚¬ìš©í•˜ì—¬ ë³¸ ë…¼ë¬¸ì˜ ë°©ë²•ì´ ì‹¤ì œ real ì´ë¯¸ì§€ì— ì ìš©ë  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆë‹¤.
        > â—† ??? real imageê°€ ë­”ë°, ì´ë•Œê¹Œì§€ ë§í•œê±´ real imageê°€ ì•„ë‹ˆì˜€ì–´?


> ì—¬ê¸°ê¹Œì§€ ì½ì–´ë³´ë©´, Cross-attention mapì€ ì‹ ì´ê³ , Diffusionì€ ë§ŒëŠ¥ì¸ë°  
> text-pixel ì‚¬ì´ì˜ semantic relationsì„ ìº¡ì³í•  ìˆ˜ ìˆë‹¤ë‹ˆ

---

## 2 Related work

- **text-driven image manipulation**ì€ ìµœê·¼ GANs [15, 8, 19â€“21] CLIP [32]ê³¼ í•¨ê»˜ ëˆˆì— ëˆ ë°œì „ì„ ì´ë£¸
    - GAN: ê³ í’ˆì§ˆ ìƒì„±ìœ¼ë¡œ ì˜ ì•Œë ¤ì§
    - CLIP: ì˜ë¯¸ì ìœ¼ë¡œ í’ë¶€í•œ joint? image-text representationìœ¼ë¡œ êµ¬ì„±, ìˆ˜ë°±ë§Œ ê°œì˜ text-image pairë¥¼ í†µí•´ í•™ìŠµ

1. **(using text only)** ì´ëŸ¬í•œ êµ¬ì„± ìš”ì†Œ(GAN, CLIP)ë¥¼ ê²°í•©í•œ íšê¸°ì ì¸(Seminal) ì—°êµ¬ë“¤[29, 14, 46, 2]ì´ í˜ì‹ ì ì´ì—ˆë˜ ì´ìœ ëŠ”,
    - ë³„ë„ì˜ ìˆ˜ì‘ì—…(extra manual labor) ì—†ì´, í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë§¤ìš° ì‚¬ì‹¤ì ì¸ ì¡°ì‘ì„ ë§Œë“¤ì–´ ëƒˆê¸° ë•Œë¬¸ 
2. **(use masks)** Bau ë“±[7]ì€ ì‚¬ìš©ìê°€ ì œê³µí•œ maskë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ê¸°ë°˜ í¸ì§‘ì„ localizeí•˜ê³ (???), íŠ¹ì • ê³µê°„ ì˜ì—­(spatial region)ìœ¼ë¡œ ë³€ê²½ì„ ì œí•œí•˜ëŠ” ë°©ë²•ì„ ì‹œì—°í•¨
    - **(í•œê³„)** GAN ê¸°ë°˜ ì´ë¯¸ì§€ í¸ì§‘ ì ‘ê·¼ ë°©ì‹ì€ ì‚¬ëŒ ì–¼êµ´ê³¼ ê°™ì€ highly-curated datasets [27]ì—ì„œëŠ” ì„±ê³µì ì´ì§€ë§Œ, í¬ê³  ë‹¤ì–‘í•œ ë°ì´í„° ì„¸íŠ¸ì—ì„œëŠ” ì–´ë ¤ì›€ì„ ê²ªìŒ
3. **(GAN)** Crowson ë“±[9]ì€ ë” í‘œí˜„ë ¥ ìˆëŠ”(expressive) generation capabilityë¥¼ ìœ„í•´ ë‹¤ì–‘í•œ ë°ì´í„°ì— ëŒ€í•´ í•™ìŠµëœ VQ-GAN[12]ì„ ë°±ë³¸ìœ¼ë¡œ ì‚¬ìš©í•¨
4. **(Diffusion)** [5, 22]ì—ì„œëŠ” ìµœì‹  Diffusion model[17, 39, 41, 17, 40, 36]ì„ í™œìš©í•˜ëŠ”ë°, ì´ëŠ” ë§¤ìš° ë‹¤ì–‘í•œ ë°ì´í„° ì„¸íŠ¸ì—ì„œ SOTAë¥¼ ë‹¬ì„±í•˜ë©°, ì¢…ì¢… GANì„ ëŠ¥ê°€í•¨[10]
5. **(global changes)** Kim ë“±[22]ì€ global changesë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤Œ 
    > â—† ??? ìŠ¤íƒ€ì¼ ë³€ê²½ ê°™ì€ê±´ê°€? ì´ê±´ ì™œ ì–¸ê¸‰í•œê±°ì„?
6. **(use masks)** Avrahami ë“±[5]ì€ ì‚¬ìš©ìê°€ ì œê³µí•˜ëŠ” ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ê³µì ì¸ local manipulationsì„ ìˆ˜í–‰í•¨


- **(text only)** ë§ˆìŠ¤í¬ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ í•„ìš”í•œ ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì€ ê¸€ë¡œë²Œ í¸ì§‘ì— êµ­í•œë˜ì–´ ìˆì§€ë§Œ[9, 23], 
    - Bar-Tal ë“±[6]ì€ ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ localized editing ê¸°ë²•ì„ ì œì•ˆí•˜ì—¬ ì¸ìƒì ì¸ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆìŒ
        - ê·¸ëŸ¬ë‚˜ ì´ ê¸°ë²•ì€ ì£¼ë¡œ í…ìŠ¤ì²˜ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆì„ ë¿ ìì „ê±°ë¥¼ ìë™ì°¨ë¡œ ë°”ê¾¸ëŠ” ê²ƒê³¼ ê°™ì€ ë³µì¡í•œ êµ¬ì¡°ë¥¼ ìˆ˜ì •í•  ìˆ˜ ì—†ìŒ
        - ë˜í•œ, ë³¸ ë…¼ë¬¸ì˜ ë°©ë²•ê³¼ ë‹¬ë¦¬ [6]ì˜ ì ‘ê·¼ ë°©ì‹ì€ ê° ì…ë ¥ì— ëŒ€í•´ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨ì‹œì¼œì•¼ í•¨
    > text onlyëŠ” ì£¼ë¡œ global ê¸°ë°˜ ì—°êµ¬[9, 23]ì´ì§€ë§Œ, local ê¸°ë°˜ì˜ text only ì—°êµ¬[6]ë„ ìˆìŒ.  
    > í•˜ì§€ë§Œ, í…ìŠ¤ì²˜ë¥¼ ë³€ê²½ë§Œ ë˜ê³ , ìì „ê±° â†’ ìë™ì°¨ ê°™ì€ê±´ ì•ˆë¨ (â—† localized editingì´ ë‚´ê°€ ìƒê°í•˜ëŠ” ê·¸ëŸ°ê²Œ ì•„ë‹Œê°€????)  
    > ë˜í•œ, ê° ì…ë ¥ì— ëŒ€í•´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë‹¤ì‹œ í›ˆë ¨í•´ì•¼ í•œë‹¤ëŠ” ì ì´ í•œê³„ë¼ê³  ë³¼ ìˆ˜ ìˆìŒ (â—† source imageë§ˆë‹¤ ë‹¤ì‹œ í•™ìŠµí•´ì•¼ í•œë‹¤ëŠ” ë§ì¸ê°€??)

- ìˆ˜ë§ì€ ì—°êµ¬[11, 16, 42, 25, 26, 30, 31, 34, 49, 9, 13, 36]ëŠ” **text-to-image synthesis**ì´ë¼ê³  ì•Œë ¤ì§„ plain textë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì´ë¯¸ì§€ ìƒì„± ê¸°ìˆ ì„ í¬ê²Œ ë°œì „ì‹œì¼°ìŒ
- **(LLI)** ìµœê·¼ Imagen [38], DALL-E2 [33], Parti [48] ë“± ì—¬ëŸ¬ large-scale text-image modelì´ ë“±ì¥í•˜ì—¬ ì „ë¡€ ì—†ëŠ” ì˜ë¯¸ ìƒì„±ì„ ë³´ì—¬ì£¼ì—ˆìŒ
    - ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ ìƒì„±ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ ì œì–´ ê¸°ëŠ¥ì„ ì œê³µí•˜ì§€ ì•Šìœ¼ë©°, íŠ¹íˆ text guidanceë§Œì„ ì‚¬ìš©í•¨
    - ì´ë¯¸ì§€ì™€ ê´€ë ¨ëœ ì›ë˜ í”„ë¡¬í”„íŠ¸ì—ì„œ ë‹¨ì–´ í•˜ë‚˜ë§Œ ë³€ê²½í•´ë„ ì™„ì „íˆ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìŒ
        - ì˜ˆë¥¼ ë“¤ì–´, 'dog'ì— í˜•ìš©ì‚¬ 'white'ì„ ì¶”ê°€í•˜ë©´ ê°œì˜ ëª¨ì–‘ì´ ë°”ë€ŒëŠ” ê²½ìš°ê°€ ë§ìŒ
    - ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ëª‡ëª‡ ì—°êµ¬[28, 4]ì—ì„œëŠ” ì‚¬ìš©ìê°€ ë³€ê²½ ì‚¬í•­ì´ ì ìš©ë˜ëŠ” ì˜ì—­ì„ ì œí•œí•˜ëŠ” ë§ˆìŠ¤í¬ë¥¼ ì œê³µí•œë‹¤ê³  ê°€ì •í•¨
- ì´ëŸ¬í•œ ê¸°ì¡´ ì—°êµ¬ì™€ ë‹¬ë¦¬, ë³¸ ë…¼ë¬¸ì€ **ìƒì„± ëª¨ë¸ ìì²´ì˜ ë‚´ë¶€ ë ˆì´ì–´ì— ìˆëŠ” ê³µê°„ ì •ë³´**ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ í…ìŠ¤íŠ¸ ì…ë ¥ë§Œ í•„ìš”í•˜ë„ë¡ í•¨ (mask X, requires textual input only)
    - ì´ëŠ” ì‚¬ìš©ìê°€ text promptë§Œ ìˆ˜ì •í•˜ì—¬ local or global ì„¸ë¶€ ì‚¬í•­(details)ì„ ìˆ˜ì •í•˜ëŠ”, í›¨ì”¬ ì§ê´€ì ì¸ editing experienceë¥¼ ì œê³µí•¨
    > â—† ì§ê´€ì ì´ë¼ëŠ” ê±´ ì•Œê² ëŠ”ë°, ì™œ ë§ˆìŠ¤í¬ë¥¼ ì“°ë©´ ì•ˆë˜ëŠ”ê°€ì— ëŒ€í•œ ê·¼ê±°ê°€ í•„ìš”í•´ ë³´ì´ë„¤

> ê° ì—°êµ¬ë¶„ì•¼ì˜ fundamental tasks ì²´í¬í•˜ê¸°  
> ì°¸.. ë­ë„ê¹Œ.. ì´ë¯¸ì§€ ìƒì„±&í¸ì§‘ ê¸°ìˆ ì€ ì´ë ‡ê²Œ ë‚ ë¡œ ë°œì „í•˜ëŠ”ë°, ì´ë¥¼ ì•…ìš©í•˜ëŠ” ë”¥í˜ì´í¬ ë¬¸ì œëŠ” ì´ëŸ¬í•œ ë°œì „ì„ ë”°ë¼ì¡ì„ ìˆ˜ ìˆëŠ”ê°€? ì´ìª½ ì—°êµ¬ë¥¼ ì•Œì•„ê°ˆ ìˆ˜ë¡ ê±±ì •ì´ ì»¤ì§€ë„¤...

---

## 3 Method

- text-guided diffusion ëª¨ë¸ `Imagen`[38]ì´ text prompt `P`ì™€ random seed `s`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±í•œ ì´ë¯¸ì§€ë¥¼ `I`ë¼ê³  ê°€ì •í•˜ì.
- (ëª©í‘œ) ë³¸ ë…¼ë¬¸ì˜ ëª©í‘œëŠ” í¸ì§‘ëœ prompt `Pâˆ—`ë¡œë§Œ guidedí•˜ì—¬, ì…ë ¥ ì´ë¯¸ì§€ `I`ë¥¼ í¸ì§‘í•œ `í¸ì§‘ëœ ì´ë¯¸ì§€ Iâˆ—`ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤.
    - ì‚¬ìš©ìê°€ ì •ì˜í•œ ë§ˆìŠ¤í¬ì— ì˜ì¡´í•˜ì§€ ì•Šìœ¼ë ¤ í•¨
    > [38] Photorealistic text-to-image diffusion models with deep language understanding [NIPS, 2022]  
        > ì´ê²Œ SOTAì˜€ë‚˜?  
        > "í…ìŠ¤íŠ¸ ì „ìš© ì½”í¼ìŠ¤ë¡œ ì‚¬ì „ í•™ìŠµëœ ì¼ë°˜ì ì¸ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(ì˜ˆ: T5)ì´ ì´ë¯¸ì§€ í•©ì„±ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì¸ì½”ë”©ì— ë†€ë¼ìš¸ ì •ë„ë¡œ íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒ"

![attention injection](\assets\img\paper\P2P_attention_injection.PNG)
_ê·¸ë¦¼ 2: Content modiï¬cation through attention injection_

```
ìœ„ìª½ í–‰: diffusion ê³¼ì • ë™ì•ˆì— ì›ë³¸ ì´ë¯¸ì§€ 'lemon cake'ì˜ attention weightsë¥¼ ì£¼ì… 
ì•„ë˜ í–‰: attention weightsë¥¼ ì£¼ì…í•˜ì§€ ì•Šê³ , ì›ë³¸ ì´ë¯¸ì§€ì™€ ë™ì¼í•œ random seedë§Œ ì‚¬ìš© (â—† ì´ê²Œ ë°‘ì—ì„œ ë§í•˜ëŠ” ì‹¤íŒ¨í•œ ì‹œë„ì¸ê°€?)
í›„ìëŠ” ì›ë³¸ ì´ë¯¸ì§€ì™€ ê±°ì˜ ê´€ë ¨ì´ ì—†ëŠ” ì™„ì „íˆ ìƒˆë¡œìš´ êµ¬ì¡°ë¥¼ ë§Œë“¦
```

- (ì‹¤íŒ¨í•œ ì‹œë„) ë‚´ë¶€ ë¬´ì‘ìœ„ì„±(internal randomness)ì„ ê³ ì •(fix)í•˜ê³  edited text promptë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì‹œ ìƒì„±í•˜ëŠ” ë°©ë²•ì€ ê°„ë‹¨í•˜ì§€ë§Œ,
    - fig. 2ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ì´ë ‡ê²Œ í•˜ë©´ êµ¬ì¡°(structure)ì™€ êµ¬ì„±(composition)ì´ ì™„ì „íˆ ë‹¤ë¥¸ ì´ë¯¸ì§€ê°€ ìƒì„±ë¨
        > â—† ì¦‰, ì´ˆê¸° ë…¸ì´ì¦ˆë¥¼ ëŒ€í‘œí•˜ëŠ” random seedë¥¼ ê³ ì •í–ˆìœ¼ë‹ˆ, ë‹¤ì‹œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆì„ ë•Œ ì›ë˜ êµ¬ì¡°ì™€ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?í–ˆëŠ”ë°,  
        > ì‹œë“œë¥¼ ê³ ì •í•˜ë”ë¼ë„ í…ìŠ¤íŠ¸ë¥¼ ë°”ê¾¸ë©´, **ëª¨ë¸ì€ ì—¬ì „íˆ ì´ë¯¸ì§€ë¥¼ ì²˜ìŒë¶€í„° ìƒˆë¡œ ìƒì„±**í•˜ê¸° ë•Œë¬¸ì—, í…ìŠ¤íŠ¸ê°€ ì¡°ê¸ˆë§Œ ë°”ê»´ë„ cross-attention ê²½ë¡œëŠ” ì™„ì „íˆ ë‹¬ë¼ì§€ê³ , êµ¬ì¡°(composition), í˜•íƒœ(geometry)**ê°€ ì „í˜€ ë‹¤ë¥´ê²Œ ë‚˜ì˜´  
        > **ì¦‰, random seedë¥¼ ê³ ì •í•œ ê²ƒë§Œìœ¼ë¡œëŠ” ì›í•˜ëŠ” ë¶€ë¶„ë§Œ ì‚´ì§ ë°”ê¾¸ëŠ” ì‹ì˜ í¸ì§‘(editing)ì€ ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒ**
- (Our key observation) ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ì ì€ 
    - ìƒì„±ëœ ì´ë¯¸ì§€ì˜ **êµ¬ì¡°(structure)ì™€ ëª¨ì–‘(appearances)**ì´ random seedë¿ë§Œ ì•„ë‹ˆë¼ **diffusion ê³¼ì • ì¤‘ì— ë‚˜íƒ€ë‚˜ëŠ” pixelê³¼ text ì„ë² ë”© ê°„ì˜ interaction**ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤ëŠ” ê²ƒ
        > í…ìŠ¤íŠ¸ê°€ ë°”ë€Œë©´ pixel-to-token ê´€ê³„ë„ ì™„ì „íˆ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— ê²°ê³¼ ì´ë¯¸ì§€ì˜ êµ¬ì¡°(structure)ì™€ ëª¨ì–‘(appearances)ì´ ê¹¨ì§€ëŠ” ê²ƒ  
        > ì¦‰, diffusionì€ **í…ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í”½ì…€ì„ êµ¬ì„±**í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆìŒ
- (ë”°ë¼ì„œ) ë³¸ ë…¼ë¬¸ì€ **cross-attention layersì—ì„œ ë°œìƒí•˜ëŠ” pixel-to-text interactionì„ ìˆ˜ì •**í•˜ëŠ” Prompt-to-Prompt image editing ì œì•ˆ
    - êµ¬ì²´ì ìœ¼ë¡œ, **ì…ë ¥ ì´ë¯¸ì§€ `I`ì˜ cross-attention mapsì„ ì‚½ì…**í•˜ë©´ ì›ë˜ì˜ êµ¬ì¡°(structure)ì™€ êµ¬ì„±(composition)ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆë‹¤.


í™•ì‚° ëª¨ë¸ì— ëŒ€í•œ ì¶”ê°€ ë°°ê²½ ì§€ì‹ì€ ë¶€ë¡ Aë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

> (ì •ë¦¬) ì´ë¯¸ì§€ IëŠ” í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ Pë¡œë¶€í„° ìƒì„±ëœ ê²°ê³¼ì´ê³ ,  
- ì´ ì´ë¯¸ì§€ ìƒì„± ì¤‘ì— ê³„ì‚°ëœ **cross-attention map M**ì€ **"ì–´ë–¤ í”½ì…€ì´ ì–´ë–¤ ë‹¨ì–´ë¥¼ ë”°ëëŠ”ê°€"ë¥¼ ë‹´ê³  ìˆìŒ**
- í¸ì§‘ëœ í”„ë¡¬í”„íŠ¸ P*ë¡œ ìƒˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ë•Œ, **ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ í† í°ì— ëŒ€í•œ attentionì€ ììœ ë¡­ê²Œ ê³„ì‚°**ë˜ë„ë¡ ë†”ë‘ê³ ,
- **ê¸°ì¡´ í† í°ì— ëŒ€í•œ attentionì€ ì´ì „ ê²ƒ(M)ì„ ê°•ì œë¡œ ë‹¤ì‹œ ì‚¬ìš©**í•˜ë©´, êµ¬ì¡°ê°€ ê¹¨ì§€ì§€ ì•ŠìŒ
    - â—† ì´ê±°, ìˆ˜ì‹ì ìœ¼ë¡œ ì¢€ ë´ì•¼ê² ë”°

---


### 3.1 Cross-attention in text-conditioned Diffusion Models

- text-conditioned Diffusion Modelsì—ì„œ cross-attentionì´ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ ë¶„ì„ ê²°ê³¼ review
> text-conditioned Diffusion Modelsì´ ë­”ë°, Imagen ê°™ì€ ëª¨ë¸ì¸ê°€?

- backbone = text-guided synthesis modelì¸ Imagen [38]
    - "Since the **composition and geometry** are mostly determined at the **64 Ã— 64 resolution**, we only adapt the text-to-image diffusion model, **using the super-resolution process as is**"
    > ìµœì‹  text-to-image diffusion ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ **ë‹¤ì¤‘ í•´ìƒë„(multi-resolution) êµ¬ì¡°**ë¥¼ ì‚¬ìš©í•¨.  
    > ë˜í•œ í˜•íƒœì  ê²°ì •(composition and geometry)ì€ ë‚®ì€ í•´ìƒë„ (64x64) ë‹¨ê³„ì—ì„œ ëŒ€ë¶€ë¶„ ì •í•´ì§.  
    > 256x256, 1024x1024 ë“±ì˜ super-resolutionì—ì„œëŠ” ì„¸ë¶€ ì •ë³´ë§Œ ë³´ê°•í•˜ê³ , êµ¬ì¡°ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ.  
    > ë”°ë¼ì„œ, ë³¸ ë…¼ë¬¸ì˜ ì·¨ì§€ì— ë§ê²Œ êµ¬ì¡°ë¥¼ ë°”ê¾¸ëŠ” ì €í•´ìƒë„ ë‹¨ê³„ë§Œ ìˆ˜ì •í•˜ê² ë‹¤.

- (Background of Diffusion step) ê° diffusion step `t`ëŠ” U-shaped ë„¤íŠ¸ì›Œí¬[37]ë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ì´ë¯¸ì§€ `z_t`ì™€ í…ìŠ¤íŠ¸ ì„ë² ë”© `Ïˆ(P)`ë¡œë¶€í„° ë…¸ì´ì¦ˆ `Îµ`ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒìœ¼ë¡œ êµ¬ì„±ë¨
    - ë§ˆì§€ë§‰ stepì—ì„œ ì´ í”„ë¡œì„¸ìŠ¤ëŠ” ìƒì„±ëœ ì´ë¯¸ì§€ `I = z_0`ì„ ì‚°ì¶œí•¨ 
        > U-shaped ë„¤íŠ¸ì›Œí¬[37] U-Net: Convolutional Networks for Biomedical Image Segmentation [MICCAI, 2015]

> â—† ê·¼ë° U-shaped network [37] ì´ ë…¼ë¬¸ì€ ì™œ ì–¸ê¸‰ëì§€? Imagenì´ ì´ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ í–ˆë‚˜?
- Imagenì—ì„œ ì‚¬ìš©í•˜ëŠ” **noise prediction network(Îµ_theta)**ê°€ U-Net êµ¬ì¡°ì´ê¸° ë•Œë¬¸
- U-Netì€ ì´ë¯¸ì§€ì˜ ì „ì²´ ì •ë³´(ì €í•´ìƒë„ ì˜ë¯¸ ì •ë³´)ì™€ ì„¸ë¶€ ì •ë³´(ê³ í•´ìƒë„ ë””í…Œì¼)ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•©í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§ 
- ê±°ì˜ **ëª¨ë“  diffusion ëª¨ë¸ (DDPM, Stable Diffusion, Imagen ë“±)**ì´ ë…¸ì´ì¦ˆ ì˜ˆì¸¡ê¸° Îµ_thetaë¥¼ U-Net êµ¬ì¡°ë¡œ êµ¬ì„±í•¨
- Prompt-to-PromptëŠ” ì´ U-Net ë‚´ë¶€ì˜ cross-attention layerë¥¼ ì¡°ì‘í•¨ìœ¼ë¡œì¨ ì‘ë™í•¨

- ê°€ì¥ ì¤‘ìš”í•œ ì ì€ ë‘ modality(text-visual) ê°„ì˜ ìƒí˜¸ ì‘ìš©(interaction)ì´ **noise ì˜ˆì¸¡ ê³¼ì •**ì¤‘ì— ë°œìƒí•œë‹¤ëŠ” ì ì¸ë°, ì´ ê³¼ì •ì—ì„œ,
    - **ê° textual tokenì— ëŒ€í•œ spatial attention map**ì„ ë§Œë“¤ì–´ë‚´ëŠ” **Cross-attention layer**ê°€ í™œìš©ëœë‹¤.
        - ì¦‰, visual featuerì™€ textual featureì˜ ì„ë² ë”©ì´ Cross-attention layerë¥¼ í†µí•´ ìœµí•©ëœë‹¤.

> â—† diffusion ëª¨ë¸ ë‚´ë¶€ì—ì„œ "í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ê°€ ì–´ë–»ê²Œ ë§Œë‚˜ëŠ”ê°€?"ì— ëŒ€í•œ ì„¤ëª…
- í…ìŠ¤íŠ¸(Text)ì™€ ì´ë¯¸ì§€(Image)ì˜ ìƒí˜¸ì‘ìš©(interaction)ì´ ì •í™•íˆ ì–´ë–¤ êµ¬ì¡°ë¥¼ í†µí•´ ì¼ì–´ë‚˜ëŠ”ì§€
- Diffusion ëª¨ë¸ì€ ê° ë‹¨ê³„ì—ì„œ ë…¸ì´ì¦ˆê°€ ì„ì¸ ì´ë¯¸ì§€ z_tì—ì„œ "ë‹¤ìŒ ìƒíƒœ z_{t-1}ë¡œì˜ ë…¸ì´ì¦ˆ Îµ"ì„ ì˜ˆì¸¡í•˜ê³  ì œê±°í•¨
    - ì´ noise prediction ë‹¨ê³„ì—ì„œ, í…ìŠ¤íŠ¸ ì„ë² ë”©ì´ í•˜ë‚œì˜ ì¡°ê±´ìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì—, í…ìŠ¤íŠ¸ê°€ ì´ë¯¸ì§€ ìƒì„±ì— ì˜í–¥(interaction)ì„ ì£¼ëŠ” ì‹œì ì´ë¼ê³  í•œê±°
- ê·¸ë¦¬ê³  ì´ ì‹œì ì—ì„œ, ëª¨ë¸ì€ "ì´ í”½ì…€ì€ ì–´ë–¤ ë‹¨ì–´ë¥¼ ì°¸ê³ í•´ì„œ ìƒì„±í• ì§€"ë¥¼ í•™ìŠµí•˜ëŠ”ë°,
    - ì´ë¯¸ì§€ì˜ latent feature Ï†(z_t)ê°€ í…ìŠ¤íŠ¸ ì„ë² ë”© Ïˆ(P)ì„ ì°¸ê³ í•´ì„œ cross-attention layerì—ì„œ ê²°í•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬ë¨
    - Cross-attentionì˜ ê²°ê³¼ë¬¼ì€ ê° í…ìŠ¤íŠ¸ í† í°ì´ ì´ë¯¸ì§€ì˜ ì–´ë–¤ ê³µê°„(spatial location)ì— ì˜í–¥ì„ ì£¼ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” attention mapì„
        - ì˜ˆë¥¼ ë“¤ì–´, "apple"ì´ë¼ëŠ” ë‹¨ì–´ì— ëŒ€í•œ attention mapì€ ì‚¬ê³¼ê°€ ìœ„ì¹˜í•  ì´ë¯¸ì§€ ì˜ì—­ì— ì§‘ì¤‘ë  ê²ƒì´ê³ ,
        - "red"ëŠ” ì‚¬ê³¼ ì „ì²´ì— í¼ì ¸ ìˆì„ ìˆ˜ë„ ìˆìŒ

![P2P Method overview](\assets\img\paper\P2P_Method_overview.PNG)
_ï¬g. 3 Method overview_

```
- Top: ê° textual tokenì— ëŒ€í•œ spatial attention mapì„ ìƒì„±í•˜ëŠ” cross-attention layerë¥¼ í†µí•´ visual ë° textual embeddingì„ ìœµí•©
    - â—† attention map Mì„ ì´ìš©í•´ì„œ textual embedding Qì™€ visual embedding K, Vë¥¼ ìœµí•©ì‹œí‚¨ë‹¤ëŠ”ê±´ê°€? K,Vê°€ ë­”ì§€ ëª¨ë¥´ê² ë„¤
- Bottom: ì†ŒìŠ¤ ì´ë¯¸ì§€ì˜ attention mapsì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ì´ë¯¸ì§€ì˜ spatial layout(êµ¬ì„±)ê³¼ geometry(í˜•ìƒ)ë¥¼ ì œì–´
    - ì´ë¥¼ í†µí•´ textual promptë§Œ í¸ì§‘í•˜ì—¬ ë‹¤ì–‘í•œ í¸ì§‘ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒ
        1) promptì—ì„œ wordë¥¼ êµì²´í•  ê²½ìš°, ì†ŒìŠ¤ ì´ë¯¸ì§€ map M_të¥¼ íƒ€ê¹ƒ ì´ë¯¸ì§€ map M_t*ë³´ë‹¤ ìš°ì„ í•˜ì—¬ ì‚½ì…í•˜ì—¬ spatial layout ìœ ì§€ 
        2) ìƒˆ ë¬¸êµ¬(phrase)ë¥¼ ì¶”ê°€í•˜ëŠ” ê²½ìš°, promptì—ì„œ ë³€ê²½ë˜ì§€ ì•Šì€ ë¶€ë¶„ì— í•´ë‹¹í•˜ëŠ” ë§µë§Œ ì‚½ì…í•©ë‹ˆë‹¤. 
            - â—† ì—¥ ë°˜ëŒ€ ì•„ë‹ˆê³ ? ì•ˆë°”ë€ prompt ë¶€ë¶„ì„ íƒ€ê¹ƒ ì´ë¯¸ì§€ map M_t*ë¡œ ë°”ê¾¼ë‹¤ê³ ? ì†ŒìŠ¤ ì´ë¯¸ì§€ map M_tê°€ ì•„ë‹ˆë¼?
        3) ë‹¨ì–´ì˜ íš¨ê³¼ë¥¼ ì¦í­/ì•½í™”ì‹œí‚¤ëŠ” ê²½ìš°, ëŒ€ì‘ë˜ëŠ” attention mapì˜ ê°€ì¤‘ì¹˜ë¥¼ re-weightingí•©ë‹ˆë‹¤.
```

- (ï¬g. 3 ì°¸ê³ ) ê³µì‹ì ìœ¼ë¡œ(formally) ë³´ë©´, 
    - **ë…¸ì´ì¦ˆê°€ ìˆëŠ” ì´ë¯¸ì§€ `Ï†(z_t)`ì˜ deep spatial feature**ëŠ” query í–‰ë ¬ `Q = ğ“_Q(Ï†(z_t))`ì— projected ë˜ê³ , 
        > `z_t`: í˜„ì¬ diffusion ë‹¨ê³„ì˜ ë…¸ì´ì¦ˆ ì´ë¯¸ì§€  
        > `Ï†(z_t)`: ê·¸ ì´ë¯¸ì§€ì— ëŒ€í•œ deep feature (CNN ê°™ì€ ì¸ì½”ë”ê°€ ì¶”ì¶œí•œ spatial feature)  
        > `ğ“_Q`: í•™ìŠµëœ ì„ í˜• ë³€í™˜(linear projection) â†’ â—† ì´ê²Œ ë­”ì§€ ëª¨ë¥´ê² ë„¤  
        > `Q` = ì´ë¯¸ì§€ì˜ ê° í”½ì…€ì€ ì¿¼ë¦¬(Query)ê°€ ë¨ â†’ "ì´ í”½ì…€ì€ ì–´ë–¤ ë‹¨ì–´ë¥¼ ì°¸ê³ í•´ì•¼ í• ê¹Œ?" ë¼ê³  ë¬»ëŠ” ê²ƒ  
    - **textual embedding**ì€ key í–‰ë ¬ `K = ğ“_K(Ïˆ(P))`ì™€ value í–‰ë ¬ `V = ğ“_V(Ïˆ(P))`ì— projectedë¨
        > `Ïˆ(P)`: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ `P`ì˜ í† í° ì„ë² ë”©  
            - ì˜ˆ: "a red apple" â†’ [a], [red], [apple]  
            - ì´ê±¸ ì„ë² ë”©í•˜ë©´ ê° ë‹¨ì–´ëŠ” ê³ ì •ëœ ë²¡í„° ê³µê°„ì˜ ì ì´ ë˜ê³ , ì´ê²Œ Ïˆ(P)  
            - ê·¸ë¦¬ê³  ê° ë‹¨ì–´(token) ì„ë² ë”©ì„ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬ K (Key), V (Value)  
            - í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì—ì„œ Key/Valueë¥¼ ë½‘ì•„ë‚´ê¸° ìœ„í•´ **ê°ê°ì˜ ì„ í˜• ë³€í™˜(linear projection)**ì„ í•™ìŠµí•˜ëŠ”ê±°
    - (í•™ìŠµëœ linear projection `ğ“_Q`, `ğ“_K`, `ğ“_V`ë¥¼ í†µí•´)
        > â—† `â„“_K(x) = W_K * x + b_K`  
        > attentionì´ ì˜ ì‘ë™í•˜ë„ë¡ diffusion loss (ex. noise prediction ì˜¤ë¥˜)ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì „ì²´ ëª¨ë¸ í•™ìŠµ ì¤‘ì— ê°™ì´ í•™ìŠµ
- ê·¸ëŸ¼ attention maps Mì€ ìˆ˜ì‹(1)ê³¼ ê°™ì•„ì§: `M = softmax(QKáµ€ / âˆšd)`  
    > `QKáµ€`: ê° ì´ë¯¸ì§€ í”½ì…€ì˜ Queryì™€, ê° ë‹¨ì–´ì˜ Key ê°„ì˜ **ìœ ì‚¬ë„(similarity)**ë¥¼ ê³„ì‚°  
    > `d`: í‚¤/ì¿¼ë¦¬ ë²¡í„°ì˜ ì°¨ì› ìˆ˜(latent projection dimension) â†’ scaleì„ ìœ„í•´ âˆšdë¡œ ë‚˜ëˆ”  
    > `softmax`: attention weightë¡œ ë°”ê¾¸ê¸° ìœ„í•´ ì •ê·œí™”  
    > `Máµ¢â±¼`: "ië²ˆì§¸ í”½ì…€ì€ jë²ˆì§¸ ë‹¨ì–´ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í•´ì•¼ í• ê¹Œ?"ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°€ì¤‘ì¹˜
- ë§ˆì§€ë§‰ìœ¼ë¡œ, cross-attention output `Ï†_hat(zt) = MV`ë¡œ ì •ì˜ë˜ë©°, ì´ëŠ” spatial feature `Ï†(zt)`ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë° ì‚¬ìš©ë¨
    > ê° í”½ì…€ì€ **ë‹¨ì–´ë³„ ê°€ì¤‘ì¹˜(M)**ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì˜ë¯¸ ì •ë³´(V)**ë¥¼ ë°›ì•„ë“¤ì„  
    > `Ï†(zt)`ëŠ” í…ìŠ¤íŠ¸ ì˜ë¯¸ê°€ ë°˜ì˜ëœ ì´ë¯¸ì§€ í”¼ì²˜ë¡œ ì—…ë°ì´íŠ¸ë¨

- ì§ê´€ì ìœ¼ë¡œ, cross-attention output MVëŠ” weightê°€ attention maps Mì¸ value Vì˜ ê°€ì¤‘ í‰ê· (weighted average)ì´ê³ , ì´ëŠ” Qì™€ K ì‚¬ì´ì˜ **similarity**ì™€ ìƒê´€ê´€ê³„ê°€ ìˆë‹¤.
- ì‹¤ì œë¡œëŠ” í‘œí˜„ë ¥(expressiveness)ì„ ë†’ì´ê¸° ìœ„í•´ multi-head attention[44]ì„ ë³‘ë ¬ë¡œ ì‚¬ìš©í•œ ë‹¤ìŒ, ê²°ê³¼ë¥¼ concatenatedí•˜ê³  í•™ìŠµëœ linear layerì„ í†µê³¼í•˜ì—¬ ìµœì¢… ì¶œë ¥ì„ ì–»ìŒ
    > multi-head attention[44]  
        - í•˜ë‚˜ì˜ attention headëŠ” í•˜ë‚˜ì˜ ë°©ì‹ìœ¼ë¡œë§Œ ì •ë³´ë¥¼ ì§‘ì¤‘í•  ìˆ˜ ìˆìŒ  
        - ì´ë¯¸ì§€ë‚˜ í…ìŠ¤íŠ¸ëŠ” ë³µì¡í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ë¯€ë¡œ, ì—¬ëŸ¬ ê°œì˜ headê°€ ë³‘ë ¬ë¡œ ê°ê¸° ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ attentionì„ ìˆ˜í–‰í•˜ê²Œ í•¨ (ë³´í†µ 8~12)  
            - ì˜ˆ: í•˜ë‚˜ëŠ” ìƒ‰ê¹”, í•˜ë‚˜ëŠ” ëª¨ì–‘, í•˜ë‚˜ëŠ” ìœ„ì¹˜ì— ì£¼ëª©í•  ìˆ˜ ìˆìŒ  
            - `â„“_Q^1, â„“_Q^2, ..., â„“_Q^h` ì´ë ‡ê²Œ ë…ë¦½ì ì¸ linear projectionì„ ì‚¬ìš©í•˜ê³ , ë…ë¦½ì ìœ¼ë¡œ attention ê³„ì‚°

```
Imagen [38], similar to GLIDE [28], conditions on the text prompt in the noise prediction of each diffusion step (see appendix A.2) through two types of attention layers: i) cross-attention layers. ii) hybrid attention that acts both as self-attention and cross-attention by simply concatenating the text embedding sequence to the key-value pairs of each self-attention layer.
```

- Imagenì€ ê° diffusion stepì—ì„œ ë…¸ì´ì¦ˆë¥¼ ì˜ˆì¸¡í•  ë•Œ, text prompt ì •ë³´ë¥¼ ì¡°ê±´(conditioning)ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ”ë°,
- ì´ conditioningì€ attention layer ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ë“¤ì–´ê°
	1. cross-attention layers
		- pixelì´ ì–´ë–¤ textì— ì£¼ëª©í• ì§€ ì •í•˜ëŠ” attention (ì•ì—ì„œ ì„¤ëª…í•œ ìˆ˜ì‹1)
	2. hybrid attention
		- self-attention + cross-attention
		- `K, V = concat(â„“_K(x), â„“_K(Ïˆ(P)))     â† [image features | text embeddings]`

```
Throughout the rest of the paper, we refer to both of them as cross-attention since our method only intervenes in the cross-attention part of the hybrid attention. That is, only the last channels, which refer to text tokens, are modiï¬ed in the hybrid attention modules.
```

- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” hybrid attentionì˜ cross-attention ë¶€ë¶„ì—ë§Œ ê°œì…í•˜ê¸° ë•Œë¬¸ì—, ìœ„ ë‘ ê°€ì§€ë¥¼ ëª¨ë‘ cross-attentionì´ë¼ê³  ë¶€ë¦„
    - ì¦‰, text tokenì„ ì°¸ì¡°í•˜ëŠ” ë§ˆì§€ë§‰ ì±„ë„ë§Œ hybrid attention ëª¨ë“ˆì—ì„œ ìˆ˜ì •ë¨
        > â—† ì—¬ê¸°ì„œ ì´ë¯¸ì§€ë¼ë¦¬ ì£¼ëª©í•˜ëŠ” ë¶€ë¶„(self-attention)ì€ ê·¸ëŒ€ë¡œ ë‘ê³ , ì´ë¯¸ì§€ê°€ í…ìŠ¤íŠ¸ë¥¼ ì°¸ì¡°í•˜ëŠ” ë¶€ë¶„(cross-attention)ë§Œ ì¡°ì‘í•˜ê² ë‹¤ëŠ” ê±°




---


### 3.2 Controlling the Cross-attention

```
We return to our key observation â€” the spatial layout and geometry of the generated image depend on the cross-attention maps. This interaction between pixels and text is illustrated in ï¬g. 4, where the average attention maps are plotted. As can be seen, pixels are more attracted to the words that describe them, e.g., pixels of the bear are correlated with the word â€œbearâ€. Note that averaging is done for visualization purposes, and attention maps are kept separate for each head in our method. Interestingly, we can see that the structure of the image is already determined in the early steps of the diffusion process.
```

- ë…¼ë¬¸ì˜ ì£¼ê´€ì°°: ìƒì„±ëœ ì´ë¯¸ì§€ì˜ **í˜•íƒœ(geometry)**ì™€ **ë°°ì¹˜(spatial layout)**ëŠ” cross-attention mapì˜ êµ¬ì„±ì— ê²°ì •ì ìœ¼ë¡œ ì˜ì¡´í•œë‹¤.
	 - ì¦‰, ì–´ë–¤ í”½ì…€ì´ ì–´ë–¤ ë‹¨ì–´ì— ì§‘ì¤‘í•˜ëŠëƒì— ë”°ë¼, ê·¸ í”½ì…€ì´ ì–´ë–¤ ê°ì²´ì˜ ì¼ë¶€ê°€ ë ì§€, ì´ë¯¸ì§€ ë‚´ ì–´ë””ì— ë°°ì¹˜ë ì§€, ì–´ë–¤ ê²½ê³„ë¥¼ í˜•ì„±í• ì§€...
- ê·¸ë¦¼4ë¥¼ ë³´ë©´, "ì–¸ì–´ì  ì˜ë¯¸ â†’ ì´ë¯¸ì§€ ê³µê°„"ì— ëŒ€í•œ ì—°ê²°ì´ attentionì„ í†µí•´ ì´ë£¨ì–´ì§ì„ ì•Œ ìˆ˜ ìˆìŒ
	- ê³°ì„ ì„¤ëª…í•˜ëŠ” ë‹¨ì–´ëŠ” ê³° ì˜ì—­ì˜ í”½ì…€ê³¼ ì—°ê²°ë¨
- í‰ê· (attention map averaging)ì€ ì‹œê°í™”ë¥¼ ì‰½ê²Œ í•˜ê¸° ìœ„í•œ ê²ƒì¼ ë¿, ì‹¤ì œ ëª¨ë¸ì€ ë‹¤ì–‘í•œ headë³„ attentionì„ ë”°ë¡œ ìœ ì§€í•˜ë©´ì„œ fine-grainedí•˜ê²Œ ì¡°ì ˆí•¨
	â†’ Multi-head attentionì˜ ì •ë³´ ì†ì‹¤ ì—†ì´ ìœ ì§€
	> ì‹œê°í™”í•´ì£¼ê¸° ìœ„í•´ì„œ, ì—¬ëŸ¬ headì˜ attentionì„ í‰ê· í•˜ì—¬ í‘œì‹œë¥¼ í–ˆë‹¤. ê·¼ë°, ì‹¤ì œë¡œëŠ” headë³„ attentionì„ ë”°ë¡œ ìœ ì§€í–ˆë‹¤.
- â˜… ì´ë¯¸ì§€ì˜ êµ¬ì¡°ê°€ í™•ì‚° ê³¼ì •ì˜ ì´ˆê¸° ë‹¨ê³„ì—ì„œ ì´ë¯¸ ê²°ì •ë˜ì–´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ
	- ì´ë¯¸ì§€ì˜ ì „ì²´ êµ¬ì„±ê³¼ í˜•íƒœ(geometry)ëŠ” ì´ˆë°˜ few steps ì•ˆì— ì´ë¯¸ ëŒ€ë¶€ë¶„ ê²°ì •ë¨
	> â˜… ì¦‰, attention injectionì„ ì´ˆë°˜ ëª‡ stepê¹Œì§€ë§Œ ì ìš©í•´ë„ êµ¬ì¡°ë¥¼ ë³´ì¡´í•  ìˆ˜ ìˆìŒ

```
Since the attention reï¬‚ects the overall composition, we can inject the attention maps M that were obtained from the generation with the original prompt P, into a second generation with the modiï¬ed prompt Pâˆ—. This allows the synthesis of an edited image Iâˆ— that is not only manipulated according to the edited prompt, but also preserves the structure of the input image I. This example is a speciï¬c instance of a broader set of attention-based manipulations leading to different types of intuitive editing. We, therefore, start by proposing a general framework, followed by the details of the speciï¬c editing operations.
```

- í•µì‹¬ ì¡°ì‘ ë°©ë²• = attention injection
    - ì›ë³¸ prompt Pì˜ cross-attention map Mì„ **ì´ˆê¸° step ë™ì•ˆ** ê°•ì œë¡œ ë‹¤ì‹œ ì‚¬ìš©í•˜ë©´
    - ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì— ë”°ë¼ **ì´ë¯¸ì§€ ë‚´ìš©ì„ ë°”ê¾¸ê¸´ í•˜ë˜**, ê¸°ì¡´ ì´ë¯¸ì§€ **êµ¬ì¡°ë¥¼ ê°€ëŠ¥í•œ ìœ ì§€**í•˜ë„ë¡ attention flowë¥¼ ì œí•œì ìœ¼ë¡œ ì œì–´ ê°€ëŠ¥í•˜ë‹¤
        > ì™œëƒ? ì´ë¯¸ì§€ì˜ ì „ì²´ êµ¬ì„±ê³¼ í˜•íƒœëŠ” ì´ˆë°˜ few stepsì˜ cross-attention mapìœ¼ë¡œ ëŒ€ë¶€ë¶„ ê²°ì •ë˜ë‹ˆê¹Œ!  
        > ì €ìë“¤ì€ ì´ëŸ°ê±¸ "ì˜ë¯¸ìˆëŠ” ì¢‹ì€ í¸ì§‘"ì´ë¼ê³  ìƒê°í•˜ëŠ” ë“¯  
        > â—† ê·¸ëŸ¼ êµ¬ì¡°ë‚˜ í˜•íƒœë¥¼ ë°”ê¾¸ê³  ì‹¶ìœ¼ë©´ ì–´ìº„?? step ìœ„ì¹˜ë¥¼ ë°”ê¾¸ë‚˜?
    - í•˜ì§€ë§Œ, ìœ„ì™€ ê°™ì€ attention injectionì€ í•˜ë‚˜ì˜ í™œìš© ì˜ˆì¼ ë¿, attention mapì€ ë” ë‹¤ì–‘í•˜ê²Œ í™œìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒ
        - íŠ¹ì • ë‹¨ì–´ ê°•ì¡°/ì•½í™” (attention reweighting), êµ¬ì¡° ì œì–´ (timestamp ì¡°ì ˆ ?) ë“±
        - ì´í›„ í¸ì§‘ ì•Œê³ ë¦¬ì¦˜ì˜ ì¼ë°˜ êµ¬ì¡° (Algorithm 1)ë¥¼ ì†Œê°œí•  ì˜ˆì •ì´ë‹¤

```
Let DM(zt, P, t, s) be the computation of a single step t of the diffusion process, which outputs the noisy image ztâˆ’1, and the attention map Mt (omitted if not used). We denote by DM(zt, P, t, s){M â† î£M} the diffusion step where we override the attention map M with an additional given map î£M, but keep the values V from the supplied prompt. We also denote by Mâˆ— t the produced attention map using the edited prompt Pâˆ—. Lastly, we deï¬ne Edit(Mt,Mâˆ— t , t) to be a general edit function, receiving as input the tâ€™th attention maps of the original and edited images during their generation.
```


```
Our general algorithm for controlled image generation consists of performing the iterative diffusion process for both prompts simultaneously, where an attention-based manipulation is applied in each step according to the desired editing task. We note that for the method above to work, we must ï¬x the internal randomness. This is due to the nature of diffusion models, where even for the same prompt, two random seeds produce drastically different outputs. Formally, our general algorithm is:
```



```
Notice that we can also deï¬ne image I, which is generated by prompt P and random seed s, as an additional input. Yet, the algorithm would remain the same. For editing real images, see section 4. Also, note that we can skip the forward call in line 7 by applying the edit function inside the diffusion forward function. Moreover, a diffusion step can be applied on both ztâˆ’1 and zâˆ— t in the same batch (i.e., in parallel), and so there is only one step overhead with respect to the original inference of the diffusion model. We now turn to address speciï¬c editing operations, ï¬lling the missing deï¬nition of the Edit(Mt,Mâˆ— t , t) function. An overview is presented in ï¬g. 3(Bottom).
```


```
[Word Swap.] In this case, the user swaps tokens of the original prompt with others, e.g., P =â€œa big red bicycleâ€ to Pâˆ— =â€œa big red carâ€. The main challenge is to preserve the original composition while also addressing the content of the new prompt. To this end, we inject the attention maps of the source image into the generation with the modiï¬ed prompt. However, the proposed attention injection may over constrain the geometry, especially when a large structural modiï¬cation, such as â€œcarâ€ to â€œbicycleâ€, is involved. We address this by suggesting a softer attention constrain: where Ï„ is a timestamp parameter that determines until which step the injection is applied. Note that the composition is determined in the early steps of the diffusion process. Therefore, by limiting the number of injection steps, we can guide the composition of the newly generated image while allowing the necessary geometry freedom for adapting to the new prompt. An illustration is provided in section 4. Another natural relaxation for our algorithm is to assign a different number of injection timestamps for the different tokens in the prompt. In case the two words are represented using a different number of tokens, the maps can be duplicated/averaged as necessary using an alignment function as described in the next paragraph.
```



```
[Adding a New Phrase.] In another setting, the user adds new tokens to the prompt, e.g., P =â€œa castle next to a riverâ€ to Pâˆ— =â€œchildren drawing of a castle next to a riverâ€. To preserve the common details, we apply the attention injection only over the common tokens from both prompts. Formally, we use an alignment function A that receives a token index from target prompt Pâˆ— and outputs the corresponding token index in P or None if there isnâ€™t a match. Then, the editing function is given by: Recall that index i corresponds to a pixel value, where j corresponds to a text token. Again, we may set a timestamp Ï„ to control the number of diffusion steps in which the injection is applied. This kind of editing enables diverse Prompt-to-Prompt capabilities such as stylization, speciï¬cation of object attributes, or global manipulations as demonstrated in section 4.
```


```
[Attention Reâ€“weighting.] Lastly, the user may wish to strengthen or weakens the extent to which each token is affecting the resulting image. For example, consider the prompt P = â€œa ï¬‚uffy red ballâ€, and assume we want to make the ball more or less ï¬‚uffy. To achieve such manipulation, we scale the attention map of the assigned token jâˆ— with parameter c âˆˆ [âˆ’2, 2], resulting in a stronger/weaker effect. The rest of the attention maps remain unchanged. That is: As described in section 4, the parameter c allows ï¬ne and intuitive control over the induced effect.
```



---

## 4 Applications


---

## 5 Conclusions


**(í•™ë¬¸ì ì¸ ë°œê²¬)**
- ì´ ì—°êµ¬ì—ì„œëŠ” í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìœµí•© ëª¨ë¸ ë‚´ì—ì„œ êµì°¨ ì£¼ì˜ ë ˆì´ì–´ì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. 
- ì´ëŸ¬í•œ ê³ ì°¨ì› ë ˆì´ì–´ëŠ” í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì˜ ë‹¨ì–´ë¥¼ í•©ì„± ì´ë¯¸ì§€ì˜ ê³µê°„ ë ˆì´ì•„ì›ƒì— ì—°ê²°í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ê³µê°„ ì§€ë„ë¥¼ í•´ì„ ê°€ëŠ¥í•œ í˜•íƒœë¡œ í‘œí˜„í•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. 
- ì´ ê´€ì°°ì„ í†µí•´ í”„ë¡¬í”„íŠ¸ì˜ ë‹¤ì–‘í•œ ì¡°ì‘ì´ í•©ì„± ì´ë¯¸ì§€ì˜ ì†ì„±ì„ ì§ì ‘ ì œì–´í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤Œìœ¼ë¡œì¨ ë¡œì»¬ ë° ê¸€ë¡œë²Œ í¸ì§‘ì„ í¬í•¨í•œ ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ë¡œ ë‚˜ì•„ê°ˆ ìˆ˜ ìˆëŠ” ê¸¸ì„ ì—´ì—ˆìŠµë‹ˆë‹¤.

**(í˜„ì‹¤ì ì¸ ì˜ì˜)**
- ì´ ì‘ì—…ì€ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ì  í˜ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ ì´ë¯¸ì§€ í¸ì§‘ ìˆ˜ë‹¨ì„ ì œê³µí•˜ê¸° ìœ„í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ì…ë‹ˆë‹¤. 
- ì´ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” í…ìŠ¤íŠ¸ë¥¼ ì¡°ì‘í•  ë•Œë§ˆë‹¤ ì›í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ì²˜ìŒë¶€í„° ìƒì„±í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê° ë‹¨ê³„ë§ˆë‹¤ ì ì§„ì ì¸ ë³€í™”ë¥¼ ë³´ì´ëŠ” ì‹œë§¨í‹± í…ìŠ¤íŠ¸ ê³µê°„ì„ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

**(í•œê³„ì )**
- ì²«ì§¸, í˜„ì¬ì˜ ë°˜ì „ í”„ë¡œì„¸ìŠ¤ëŠ” ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì—ì„œ ëˆˆì— ë„ëŠ” ì™œê³¡ì„ ì´ˆë˜í•©ë‹ˆë‹¤. 
    - ë˜í•œ ë°˜ì „í•˜ë ¤ë©´ ì‚¬ìš©ìê°€ ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ë³µì¡í•œ êµ¬ë„ì˜ ê²½ìš° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - í…ìŠ¤íŠ¸ ìœ ë„ í™•ì‚° ëª¨ë¸ì— ëŒ€í•œ ë°˜ì „ ë¬¸ì œëŠ” ì•ìœ¼ë¡œ ì² ì €íˆ ì—°êµ¬í•´ì•¼ í•  ê³¼ì œì…ë‹ˆë‹¤. 
- ë‘˜ì§¸, í˜„ì¬ ê´€ì‹¬ë„ ë§µì€ êµì°¨ ê´€ì‹¬ë„ê°€ ë„¤íŠ¸ì›Œí¬ì˜ ë³‘ëª© ì§€ì ì— ìœ„ì¹˜í•˜ê¸° ë•Œë¬¸ì— í•´ìƒë„ê°€ ë‚®ìŠµë‹ˆë‹¤. 
    - ì´ë¡œ ì¸í•´ ë”ìš± ì •ë°€í•œ ë¡œì»¬ë¼ì´ì¦ˆë“œ í¸ì§‘ì„ ìˆ˜í–‰í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. 
    - ì´ë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ ê³ í•´ìƒë„ ë ˆì´ì–´ì—ë„ í¬ë¡œìŠ¤ ì–´í…ì…˜ì„ í†µí•©í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤.
    - ì´ ì‘ì—…ì€ í˜„ì¬ ì €í¬ì˜ ì—­ëŸ‰ì„ ë²—ì–´ë‚œ í›ˆë ¨ ì ˆì°¨ë¥¼ ë¶„ì„í•´ì•¼ í•˜ë¯€ë¡œ í–¥í›„ ì‘ì—…ìœ¼ë¡œ ë‚¨ê²¨ë‘ê² ìŠµë‹ˆë‹¤. 
- ë§ˆì§€ë§‰ìœ¼ë¡œ, í˜„ì¬ ë°©ë²•ìœ¼ë¡œëŠ” ì´ë¯¸ì§€ì—ì„œ ê¸°ì¡´ ì˜¤ë¸Œì íŠ¸ë¥¼ ê³µê°„ì ìœ¼ë¡œ ì´ë™í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ì—†ìŒì„ ì¸ì‹í•˜ê³  í–¥í›„ ì‘ì—…ì„ ìœ„í•´ ì´ëŸ¬í•œ ì¢…ë¥˜ì˜ ì œì–´ë„ ë‚¨ê²¨ë‘ì—ˆìŠµë‹ˆë‹¤.


---

## A Background

```
- auto-regressive modelê³¼ diffusion modelì„ í¬í•¨í•œ SOTA ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ
- large-scale language-image (LLI) models (ëŒ€ê·œëª¨ ì–¸ì–´ ì´ë¯¸ì§€ ëª¨ë¸): Imagen [38], DALLÂ·E 2 [33] and Parti [48]
- ì´ë¯¸ì§€ í¸ì§‘ ì—°êµ¬? LLI-based methods [28, 4, 33]

```

### A.1 Diffusion Models


### A.2 Cross-attention in Imagen

---

## B Additional results