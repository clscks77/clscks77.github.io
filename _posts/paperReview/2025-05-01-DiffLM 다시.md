---
title: "[Gen] DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models 다시"
categories: [PaperReview, GenerativeAI]
tags: [latent, llm, diffussion, synthetic_data]
date: 2025-05-01 23:47:06 +0900
comments: false
excerpt: "DiffLM 리뷰 이어서.. VAE + diffusion으로 latent space를 정의하고, 이를 LLM에 plug-and-play로 주입"
--- 
---

전문성을 올리자! 문제점 위주로 정리


> 어떤 문제를 해결하기 위해, 어떤 방법을 택했고, 어떤 알고리즘을 썼는가에 대한 자세한 분석

> ◆ 아, 원래 naive? Diffusion 모델이 뭔지를 알고 봐야, 차별점을 파악할 수 있을 듯

> Background: Diffusion, VAE, 

- (문제) 프롬프트 LLM을 통한 합성 데이터 생성
    - LLM이 대상 데이터 분포를 이해하지 못하는 한계
    - 특히 구조화된 형식의 데이터에 대한 프롬프트 엔지니어링의 복잡성 
        - 실제 데이터 분포와 괴리(Distribution Shift)를 없애야 함

- (방법) DiffLM이 선택한 방법
    - VAE의 잠재 표현과 실제 데이터 분포 사이에 상당한 불일치가 발견됨에 따라
        > ◆ 이 부분이 궁금하다
    - Latent Diffusion 모듈을 프레임워크에 도입하여 **완전한 표현력을 갖춘** 잠재 분포를 학습
        > ◆ 이 부분이 궁금하다

- (알고리즘) Latent Variable Model (LVM; 잠재변수모델)
    - 이미지 생성 분야에서 큰 성과를 보임
    - 특히 잠재 확산 모델은 Dada space가 아닌 Latent space에서 확산 과정을 수행하여 생성 품질과 계산 효율성 간의 균형을 최적화함
    - NLP 분야에서도 Latent space를 활용하는 시도가 있었음
        - 문장 표현, 데이터 세트 증강 등의 작업을 수행하기 위해 잠재 공간을 언어 모델과 결합하려는 시도  
        (Wiseman et al., 2018; Ding &Gimpel, 2019; Li et al., 2022; Gu et al., 2023; Borisov et al., 2023)


★잠재 지식 주입(latent knowledge injection)의 여러 방법

------------------------

(Section 3.2) 불연속 데이터를 연속 Latent Space에 매핑하는 인코더
(Section 3.3) text feature가 추출 및 압축되더라도, VAE의 Vanilla Latent Embedding은 Latent Space의 활용도가 낮거나(underutilized)  
Latent Space에 빈 영역(empty regions)이 있어 종종 디코딩에 실패함.
→ 본 논문은 해당 문제를 해결하기 위해 Latent Diffusion Model을 채택함



DiffLM의 주요 구성 요소
1. VAE Encoder (학습 가능 LM 사용):
    - 실데이터 x를 continuous latent space z₀로 인코딩.
    - 여기서 VAE는 단순한 가우시안 분포에 국한되지 않고 복잡한 분포를 표현하려 함.

2. Latent Diffusion Module:
    - 기존 VAE latent space가 비어 있거나 활용도가 낮은 문제를 해결.
    - Latent space 상에서 노이즈를 추가하고 denoising 과정을 통해 보다 정밀한 latent 분포를 학습.
    - Stable Diffusion 같은 이미지 생성 모델에서 성공한 기법을 텍스트 생성에 도입.

3. Latent Feature Injector with Soft Prompting:
    - 학습된 latent vector를 LLM의 decoding 단계에 soft하게 주입.
    - 이를 통해 LLM이 latent space의 정보를 바탕으로 제어 가능한 결과물을 생성.


------------------------


### 3.2 VAE-BASED REPRESENTATION LEARNING

- 불연속 데이터를 연속 Latent Space에 매핑하는 인코더
    - **"데이터 분포를 학습하는 인코더"**와 **"언어 생성을 담당하는 디코더(LLM)"**를 분리함으로써, 두 모델 각각의 강점을 최대한 유지하면서 결합

| 구성 요소 | 기능 | 목적 |
|-----------|------|------|
| Pre-trained Transformer 인코더 | 텍스트 → \( \mu, \sigma \) | 의미론적 정보 추출 |
| Reparameterization trick | \( \mu, \sigma \) → \( \mathbf{z} \) | 확률적 샘플링 |
| Frozen LLM decoder | \( \mathbf{z} \) → 텍스트 복원 | 사전 지식 재활용 |
| β-VAE 학습 전략 | \( \text{ELBO}_\beta \) 최소화 | 균형 있는 latent space 정규화 |

- Feature Encoding (VAE의 Encoder 역할; Transformer LM): 
    - 구조화된 텍스트 데이터 `si`를 input으로 함
        - 예: tabular 구조의 문자열 `{age: 38, income: <=50K, ...}` 
    - 학습 가능한 Transformer 기반의 사전 훈련된 언어 모델(Vaswani et al., 2017; Devlin et al., 2019; Raffel et al., 2020)을 활용하여
        - Pre-trained Transformer LM 사용 (예: BERT, T5 등 가능)
    - 평균µ과 분산σ으로 나눌 수 있는 representation vector `xi`를 얻음
        - x i ​ ∈R d×2  -- 여기서 2는 각각 평균 µ, 분산 σ를 나타냄
    - re-parameterization trick (Kingma &Welling, 2014)을 이용해서 latent feature z를 얻음     `z = µ + σ ⊙ ϵ, ϵ∼N(0,I)`
        - z는 ~에서 샘플링된 latent 벡터
        - z를 VAE 구조로 학습하여 좋은 latent representation을 만들고, 이후 diffusion에서 사용
        > ◆ ??? 확률적 인코딩을 통해 다양한 z 샘플을 생성함으로써, 데이터 다양성 확보 = 단순 복제 방지

- LLM Decoding (VAE의 Decoder 역할; Frozen LLM):
    - latent feature z를 생성한 후, **frozen-parameter LLM**을 사용하여 causal language modeling manner로 입력 텍스트 s를 다시 구성
        - LLM 파라미터를 동결하는 이유는 재학습을 피하고 일반적인 지식과 기능을 보존하기 위해
        - 결과적으로 latent space와 LLM input space의 두 가지 modalities를 aligning하는 것이 새로운 문제가 된다.
    - ◆ 이를 해결하기 위해 소프트 프롬프트를 사용하는 새로운 잠재 특징 인젝터를 제안하고 해당 인젝터 네트워크를 설계하였다?

- VAE Training Objective: **Evidence Lower Bound (ELBO)**
    - VAE 모델은 일반적으로 증거 하한선(ELBO) 손실 함수를 사용하여 훈련됨
    - 이전 연구(Burgess et al., 2018)에 따라, 저자들은 ```β-VAE 훈련``` 전략을 채택
        - total loss function에서 KL divergence loss의 영향을 제어하기 위해 가중치 매개 변수 β를 도입
            - 구체적으로 β = 0인 경우 모델은 standard autoencoder로 축소되고,
            - β > 0인 경우, KL constraint는 더 부드러운 latent space를 학습하도록 장려됨
    - 식에서 pθ(x|z)는 language modeling reconstruction likelihood, qϕ(z|x)는 근사 위치(approximate posterior), p(z)는 잠재 공간에 대한 prior (=Gaussian distribution)
    - 모델을 설계할 때, latent diffusion의 denoising network(노이즈 제거?)를 고려하여 decreasing β adjustment 전략을 채택했음
        - 처음에는 큰 β 가중치를 설정하여 latent space에 강력한 정규화(regularization) 적용
        - reconstruction loss의 수렴이 느려지면 β 값을 줄여 모델이 재구성 정확도에 더 집중할 수 있도록 함
    - 또한, 과적합을 방지하기 위해 early stopping mechanism을 사용함
    > 즉, 적용된 학습법이, β-VAE 기반 ELBO = 잠재 공간을 정보 손실 없이 정형화하기 위한 의도된 조절
    > 여기에 β 스케줄링 전략으로, 
    - 학습 초반: 큰 β로 시작해 KL term을 강하게 -> 잠재 공간 정규화
    - 후반: β를 점진적으로 감소 → 복원 정확도 중심으로 학습 유도
    - 조기 종료(Early stopping): 오버피팅 방지

---

1. Reparameterization trick이 뭐지? si의 평균 µ, 분산 σ을 Pre-trained Transformer LM에서 뽑아내고, 이걸 정규분포에 element-wise하면 다양한 z 샘플을 생성 가능하다? 이해가 잘 안돼.
    - 
2. causal language modeling이 머지?
    - Causal Language Modeling = 문장을 왼쪽에서 오른쪽으로 읽으며 다음 단어를 예측하는 조건부 확률 모델을 학습하는 GPT 방식
3. 모달리티(modalities)가 불일치한다는게 뭐지? 
    - 모달리티(modality) = 데이터의 표현 형식, 즉 정보의 표현 수단
    - latent space는 실수 벡터 공간 (continuous vector) /  LLM 입력은 텍스트 시퀀스 (tokenized discrete space) → 형태와 의미가 다름
    - 그래서 latent vector를 soft prompt 형태로 변환해서 LLM에 주입하는 중간층이 필요
4. "VAE Training Objective"에서 말하는 VAE 모델은 이 논문의 전체 아키텍처에서 어느 부분을 말하는거지? Encoding에 쓰이는 VAE?
    - 그러니까, Transformer LM가 여기서 말하는 VAE인건가? Transformer는 s에서 평균, 표준편차를 추출해주는 부분이잖아.
5. KL divergence loss? KL constraint? 이게 뭐지? 이 개념이 왜 나왔지?
    - KL divergence (Kullback-Leibler divergence) = 두 확률 분포의 차이를 측정하는 척도
    - Dkl(인코더가 추정한 posterior 분포 || 우리가 원하는 prior 분포)
        - 이 값이 작을 수록 latent space가 일관되고 깔끔한 prior 분포(정규분포) 형태로 정렬됨
6. VAE Training Objective 부분을 다시 자세히 살펴보자.
    - 즉, latent encoder는 β-VAE 기반 ELBO 손실 함수로 훈련됨
    - VAE는 **Evidence Lower Bound (ELBO)**를 최대화함
        - ELBo = 복원 손실(reconstruction likelihood; 입력 x를 z로부터 잘 복원하는가?) - 정규화(KL divergence; latent 분포가 정규분포와 얼마나 가까운가?)
    - β-VAE란? 기존 ELBO에서 KL 항에 가중치 β를 도입한 것. β = 0 → 복원을 잘하자, 1 → 정규화를 잘하자
7. 여기서 말하는 latent diffusion의 노이즈가 정확히 뭐지?
    - 가우시안 노이즈

---

### 3.3 LATENT SPACE DENOISING

- VAE로 얻은 latent space의 불완전성 문제를 해결하기 위해 diffusion model을 도입하는 과정

- (문제점) VAE로 샘플(z)을 뽑아도 좋은 데이터를 생성한다는 보장이 없다.
    - 왜냐면 실제로 학습된 posterior 분포 qϕ ​(z∣x)가 정규분포 prior p(z)와 꽤 다르기 때문 (◆ 왜 굳이 정규분포여야 하지?)
    - 즉, prior에서 샘플링하면 비현실적 문장 등의 질 낮은 결과물이 생성됨
    - 따라서 latent 공간을 더 정교하게 학습하기 위해 diffusion을 도입

- VAE가 데이터의 latent space representation을 학습 = qϕ ​(z∣x)를 학습?

- VAE는 데이터의 잠재 공간 표현을 학습할 수 있지만, 사전 분포 p(z)에서 직접 샘플링하면 생성된 샘플의 품질이 떨어지는 경우가 많습니다. 예비 실험에서 VAE가 학습한 잠재 특징을 직접 활용하면 목표 데이터 분포와 무관한 텍스트가 자주 생성되는 것을 관찰했습니다. 이 문제는 인코더가 학습한 사후 분포 qϕ(z|x)와 사전 분포 p(z) 사이의 불일치로 인해 발생합니다. 이 문제를 해결하기 위해 잠재 공간에 확산 모델을 도입하여 잠재 특징의 실제 분포를 보다 정확하게 모델링합니다.

-  Zhang 등(2024)에서 영감을 받아 각 데이터 포인트 x ∈ Dtrain에 대해 훈련된 VAE에서 잠재 벡터 z ∈ Z를 추출합니다. 초기 잠재 벡터 z0에서 시작하여 선형 일정에 따라 시간이 지남에 따라 점진적으로 노이즈를 추가하여 zt를 얻습니다. 역확산 과정에서 표준 연속 노이즈 제거 네트워크를 사용하여 z0을 복구합니다(Song et al., 2021). 훈련 목표를 위해 노이즈 제거 점수 매칭을 통해 확산 모델을 최적화합니다(Karras et al., 2022): 

- 순방향 프로세스 식 5에서 zt는 시간 t의 잠재 변수이고 σ(t)는 시간 의존적 노이즈 스케일 함수입니다. 
- 역방향 프로세스 식 6의 경우 ˙σ(t)는 σ(t)의 시간 미분을 나타내고,∇zt log p(zt)는 zt에 대한 로그 확률 밀도의 기울기로 점수 함수라고도 하며, dωt는 Wiener 프로세스(표준 브라운 운동)의 증분입니다. 
- 확산 모델 훈련 손실 방정식 7에서 ϵθ(zt, t)는 주어진 노이즈 ϵ를 예측하는 신경망으로, 확산 모델에 대한 자세한 설명은 부록 A.1에서 확인할 수 있습니다.


----

1. 문제를 다시 정리해보자. 학습된 posterior q가 원래 데이터인 X의 분포인거지? 이걸 Loss_kl을 이용해서 정규분포 p(z)에 가까워지도록 학습하는거 아니였나? 근데 왜 갑자기 q와 p가 다르기 때문에 질 낮은 결과물이 나온다고 하지? 인코더 학습의 목표가 원래 계획과 안맞는 것 같은데? 내가 어디를 잘못 이해한거지?
    - q는 x로부터 얻은 잠재 분포, p는 우리가 샘플링하고 싶은 이상적인 분포
2. VAE가 데이터의 latent space representation을 학습 = qϕ ​(z∣x)를 학습? 이렇게 이해하는게 맞나?
     - qϕ ​(z∣x)는 **입력 x**를 잠재공간 z로 변환하는 확률적 인코더로, 이 분포의 평균과 분산을 통해 latent 표현을 얻음
     - VAE의 핵심은 이 분포를 잘 정리된, 유의미한 형태로 학습하는 것
3. Diffusion에서는 re-parameterization trick으로 나온 z를 zt까지 더 정규분포로 만들어버리고 있는건가? 이 상태에서 denoizing을 하면 더 좋은 표현이 나온다? 이게 어떻게 실제 데이터와 더 유사한 의미 공간 표현이 되지?

```
1. 사실 애초에 왜 정규분포로 한 번 갔다와야 하는건지 모르겠어. p가 왜 샘플링하고 싶은 이상적인 분포이지? 우리는 q 분포와 같은 다양한 데이터를 생성하고 싶은거 아니야? 이게 현실적인 데이터를 만드는거잖아.
2. qϕ ​(z∣x)는 복잡하고 비선형적이라 새로운 z를 만들기 어렵다?? 무슨 말이지??
3. "노이즈에서 진짜처럼 보이는 z 생성 가능. 즉, 진짜 분포를 더 잘 샘플링할 수 있게 됨" 왜??? 무슨 말이지???
4. 아, 혹시 VAE는 정규분포까지 안가고 단지 "근사" 해버리기 때문에 좋은 샘플링이 안나온다는거야? 대체 왜 완벽한 정규분포에 도달하면 좋은 샘플링이 나오는거지?????
5. 지금 내 질문들을 보면 내가 어떤걸 잘모르는 상태인건지 알겠어?
```

> **우리가 진짜 원하는 건 현실적인 데이터를 생성하는 건데, 왜 자꾸 z를 정규분포처럼 만들고, 그 정규분포에서 샘플을 뽑으려고 하지? 그냥 q(z|x)에서 하면 안 돼?**
- q(z|x)는 한 가지 입력에 대한 분포일 뿐! 실제 전체 데이터를 반영하는 일반적인 분포가 아니다!
- 일반적인(전체 데이터를 대표할 수 있는) 분포인 p(z)를 정해두고, 그걸 기반으로 샘플해야 함
- 즉, q는 학습에만 쓰이는 분포, p는 생성할 때 쓰기 위한 분포
- 그리고 diffusion은 q(z|x)에서 직접 z_0들을 수집해서, 그 분포 전체를 정규분포에 맞춰가며 학습
    - 아니 이걸 하고있던게 아니란 말이야??? 왜 인코더에서 이걸 안하는건데. 동시에 못하나?

> VAE는 왜 그걸 못하나?★
- VAE는 q(z|x) ≈ p(z)만 근사할 뿐, 실제로 q가 어떻게 생겼는지 전부 파악하진 못한다!
- q(z|x)가 복잡한 모양을 가지고 있어도, VAE는 **단순한 KL로만 정규분포에 "끌어당기기"**만 할 뿐
- 즉, 정규분포와 실제 z 분포가 비슷하지 않으면, VAE는 생성에서 실패
    - ◆즉, VAE는 음.... VAE와 Diffusion의 차이점이 뭔지 모르겠네? 둘 다 정규분포로 갔다가 원래 상태로 돌아오는 방법을 학습하는거 아닌가?
    - ◆그럼 이제 VAE는 대체 왜 그런짓을 하는가가 궁금해지는데. VAE가 하는 행동은 유용한 의미가 있나?



> 이를 저자들이 설명하기로는,
- z-space를 완전히 자유롭게 두면 복잡하고 구멍이 많은 공간이 됨 → 샘플링했을 때 이상한 z가 나옴 → 정규분포로 설정하면 안정화 가능

> Diffusion의 역할
- 좋은 z라고 판단하는 것들을 직접 모아서, 정규분포로 노이즈를 주입했다가, 그걸 다시 z_0로 복원하는 방법을 학습
- 따라서, 정규분포에서 샘플을 뽑더라도 학습된 역방향 과정을 통해 "좋은 z_0"으로 되돌릴 수 있음 멋지다진짜
- 그래서 **"정규분포에서 출발한 z도 좋은 의미 공간이 된다"**는 것



- ELBO 최적화는 정보이론적 해석



----

> 하, Diffusion 논문부터 봐야겠는데
- Auto-Encoder > Variational Auto-Encoder(VAE) > Diffusion 
    - 인코더가 평균, 분산을 출력하고, 이 가우시안 분포에서 랜덤하게 z를 샘플링하여 디코더로 입력


### 3.4 LATENT FEATURE INJECTION

- 학습된 latent vector z를 LLM에 어떻게 효과적으로 전달하는가 (modality 문제)
- LLM의 기존 성능을 해치지 않은 채로 latent 정보를 추가하고 싶음
- 기존의 adapter 학습 방식에서 영감을 받아 Soft Prompt 형태로 z를 주입
- soft prompt 외에도 기존 주입 방법 두 가지(Li et al., 2020 제안)를 비교 실험했으나, Soft prompt 방식이 가장 낮은 복원 손실 및 다운스트림 작업 성능을 보임
    - Prompt Injection: soft tokens을 input 맨 앞에 삽입
    - Memory Injection: 각 attention layer의 KV memory로 삽입
    - Embedding Injection: z를 직접 token embedding에 더함


- “Soft Prompt”란, 텍스트 기반이 아니라 직접 학습된 가짜 임베딩 벡터들의 집합
- 데이터의 정보를 압축 요약한 Latent Vector z를 MLP를 통해 k개의 token embedding으로 변환
    - 즉, 하나의 벡터를 여러 개의 벡터로 확장시킨 것
- 요약된 z 정보를 기반으로 LLM이 이해할 수 있는 가상의 문장 서두를 만들어주는 것



표 1: 생성된 표 형식의 데이터를 사용한 다운스트림 작업의 성능. 머신 러닝 효율성(MLE) 작업의 성능과 열 단위 분포 밀도 추정(ρ) 작업에서 품질을 평가합니다. 
↑, ↓는 지표가 높을수록(또는 낮을수록) 더 나은 성능을 나타냅니다. 
굵은 글씨는 언어 모델에 기반하여 DiffLM이 SoTA 모델을 능가함을 나타냅니다. 
빨간색 굵은 글씨는 실제 데이터를 사용하여 달성한 MLE 성능을 DiffLM이 초과함을 나타냅니다.



----

## My research

- 일단, DiffLM은 말그대로 VAE + Diffusion이고, 이걸 LLM에 넣을 때, prompt injection을 했다 정도? 근데 이걸 잘 결합했음.
    - 덕분에 VAE랑 Diffusion 공부 조금 했음
- DiffLM은 범용인거고, 나는 범용일 필요 없으니까 완전 domain 특화해서 데이터 신뢰도를 99.9%로 만들고 싶음







